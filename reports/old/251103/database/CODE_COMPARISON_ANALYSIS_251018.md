# ê³„íšì„œ vs ì‹¤ì œ ì½”ë“œ ëŒ€ì¡° ë¶„ì„

**ì‘ì„±ì¼**: 2025-10-18
**ëª©ì **: SEARCH_QUALITY_IMPROVEMENT_PLAN_251018.md ê³„íšì„œ ê²€ì¦

---

## âœ… ì‹¤ì œ ì½”ë“œ í™•ì¸ ê²°ê³¼

### 1. í˜„ì¬ ìƒíƒœ í™•ì¸

#### **hybrid_legal_search.py í˜„í™©**

**Line 28-29: ChromaDB Import** âœ… ê³„íšì„œ ì •í™•
```python
import chromadb
from chromadb.config import Settings
```
â†’ **í™•ì¸**: ChromaDB ì‚¬ìš© ì¤‘ (FAISS ì•„ë‹˜)

**Line 48-50: __init__ íŒŒë¼ë¯¸í„°** âœ… ê³„íšì„œ ì •í™•
```python
def __init__(
    self,
    sqlite_db_path: Optional[str] = None,
    chroma_db_path: Optional[str] = None,  # â† ChromaDB
    embedding_model_path: Optional[str] = None,
    collection_name: str = "korean_legal_documents"  # â† ChromaDB ì»¬ë ‰ì…˜
):
```
â†’ **í™•ì¸**: chroma_db_path ì‚¬ìš© ì¤‘

**Line 62-64: Configì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°** âœ… ê³„íšì„œ ì •í™•
```python
self.sqlite_db_path = sqlite_db_path or str(Config.LEGAL_PATHS["sqlite_db"])
self.chroma_db_path = chroma_db_path or str(Config.LEGAL_PATHS["chroma_db"])
self.embedding_model_path = embedding_model_path or str(Config.LEGAL_PATHS["embedding_model"])
```
â†’ **í™•ì¸**: Config.LEGAL_PATHS["chroma_db"] ì‚¬ìš©

**Line 69: ChromaDB ì´ˆê¸°í™” í˜¸ì¶œ** âœ… ê³„íšì„œ ì •í™•
```python
self._init_chromadb()
```
â†’ **í™•ì¸**: FAISSê°€ ì•„ë‹Œ ChromaDB ì´ˆê¸°í™”

**Line 84-95: _init_chromadb() ë©”ì„œë“œ** âœ… ê³„íšì„œ ì •í™•
```python
def _init_chromadb(self):
    """ChromaDB ì´ˆê¸°í™”"""
    try:
        self.chroma_client = chromadb.PersistentClient(
            path=self.chroma_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        self.collection = self.chroma_client.get_collection(self.collection_name)
        logger.info(f"ChromaDB loaded: {self.chroma_db_path} ({self.collection.count()} documents)")
    except Exception as e:
        logger.error(f"ChromaDB initialization failed: {e}")
        raise
```
â†’ **í™•ì¸**: ChromaDB ì´ˆê¸°í™” ë¡œì§

**Line 229: ì¿¼ë¦¬ ì„ë² ë”©** âœ… ê³„íšì„œ ì •í™•
```python
query_embedding = self.embedding_model.encode(query, convert_to_tensor=False).tolist()
```
â†’ **í™•ì¸**: ì¿¼ë¦¬ ê·¸ëŒ€ë¡œ ì„ë² ë”© (ì „ì²˜ë¦¬ ì—†ìŒ)

**Line 240: ChromaDB ê²€ìƒ‰** âœ… ê³„íšì„œ ì •í™•
```python
results = self.collection.query(**search_params)
```
â†’ **í™•ì¸**: ChromaDB collection.query() ì‚¬ìš©

**Line 369-373: search_specific_article()ì—ì„œ ChromaDB ì‚¬ìš©** âœ… ê³„íšì„œ ì •í™•
```python
# ChromaDBì—ì„œ chunk ë‚´ìš© ì¡°íšŒ
chunk_ids = self.get_chunk_ids_for_article(article["article_id"])

chunks = []
if chunk_ids:
    try:
        chroma_results = self.collection.get(ids=chunk_ids)
        if chroma_results and chroma_results["documents"]:
            chunks = chroma_results["documents"]
```
â†’ **í™•ì¸**: ChromaDB collection.get() ì‚¬ìš©

---

#### **config.py í˜„í™©**

**Line 46-50: LEGAL_PATHS** âœ… ê³„íšì„œ ì •í™•
```python
LEGAL_PATHS = {
    "chroma_db": LEGAL_INFO_BASE / "chroma_db",                          # ChromaDB vector database
    "sqlite_db": LEGAL_INFO_BASE / "sqlite_db" / "legal_metadata.db",   # SQLite metadata
    "embedding_model": BASE_DIR / "app" / "ml_models" / "KURE_v1",  # Korean Legal Embedding Model
}
```
â†’ **í™•ì¸**:
- `"chroma_db"` ê²½ë¡œë§Œ ìˆìŒ âœ…
- `"faiss_db"` ê²½ë¡œ ì—†ìŒ âœ…
- ê³„íšì„œ ë¶„ì„ ì •í™•!

---

## ğŸ“‹ ê³„íšì„œ ê²€ì¦ ê²°ê³¼

### Phase 1: ChromaDB â†’ FAISS ì½”ë“œ ì „í™˜

#### **1-1. Import ë³€ê²½** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ ì½”ë“œ**:
```python
# í˜„ì¬ (Line 28-30)
import chromadb
from chromadb.config import Settings

# ë³€ê²½ í›„
import faiss
import pickle
import numpy as np
```

**ì‹¤ì œ ì½”ë“œ í™•ì¸**: Line 28-29
```python
import chromadb
from chromadb.config import Settings
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì •í™•í•¨**
- í˜„ì¬ ChromaDB import ì‚¬ìš© ì¤‘
- FAISS import ì—†ìŒ
- ê³„íšì„œ ì œì‹œ ë‚´ìš©ê³¼ ì¼ì¹˜

---

#### **1-2. ì´ˆê¸°í™” ë©”ì„œë“œ ë³€ê²½** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ - í˜„ì¬ ì½”ë“œ**:
```python
def __init__(
    self,
    sqlite_db_path: Optional[str] = None,
    chroma_db_path: Optional[str] = None,
    embedding_model_path: Optional[str] = None,
    collection_name: str = "korean_legal_documents"
):
    self.chroma_db_path = chroma_db_path or str(Config.LEGAL_PATHS["chroma_db"])
    self.collection_name = collection_name

    # ì´ˆê¸°í™”
    self._init_sqlite()
    self._init_chromadb()  # â† ì´ê±°
    self._init_embedding_model()
```

**ì‹¤ì œ ì½”ë“œ í™•ì¸**: Line 45-72
```python
def __init__(
    self,
    sqlite_db_path: Optional[str] = None,
    chroma_db_path: Optional[str] = None,
    embedding_model_path: Optional[str] = None,
    collection_name: str = "korean_legal_documents"
):
    # Configì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    self.sqlite_db_path = sqlite_db_path or str(Config.LEGAL_PATHS["sqlite_db"])
    self.chroma_db_path = chroma_db_path or str(Config.LEGAL_PATHS["chroma_db"])
    self.embedding_model_path = embedding_model_path or str(Config.LEGAL_PATHS["embedding_model"])
    self.collection_name = collection_name

    # ì´ˆê¸°í™”
    self._init_sqlite()
    self._init_chromadb()
    self._init_embedding_model()

    logger.info("HybridLegalSearch initialized successfully")
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì •í™•í•¨**
- `chroma_db_path` íŒŒë¼ë¯¸í„° ìˆìŒ
- `collection_name` íŒŒë¼ë¯¸í„° ìˆìŒ
- `_init_chromadb()` í˜¸ì¶œ
- ê³„íšì„œì™€ ì™„ì „ ì¼ì¹˜

**ê³„íšì„œ ì œì‹œ - ë³€ê²½ í›„ ì½”ë“œ**:
```python
def __init__(
    self,
    sqlite_db_path: Optional[str] = None,
    faiss_db_path: Optional[str] = None,  # â† ë³€ê²½
    embedding_model_path: Optional[str] = None
):
    self.faiss_db_path = faiss_db_path or str(Config.LEGAL_PATHS["faiss_db"])

    # ì´ˆê¸°í™”
    self._init_sqlite()
    self._init_faiss()  # â† ë³€ê²½
    self._init_embedding_model()
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ ì½”ë“œ ì ì ˆí•¨**
- `chroma_db_path` â†’ `faiss_db_path` ë³€ê²½
- `collection_name` ì œê±° (FAISSëŠ” ì»¬ë ‰ì…˜ ê°œë… ì—†ìŒ)
- `_init_chromadb()` â†’ `_init_faiss()` ë³€ê²½
- ë…¼ë¦¬ì ìœ¼ë¡œ ì˜¬ë°”ë¦„

---

#### **1-3. FAISS ì´ˆê¸°í™” ë©”ì„œë“œ ì¶”ê°€** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ ì½”ë“œ**:
```python
def _init_faiss(self):
    """FAISS ì´ˆê¸°í™”"""
    try:
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        faiss_index_path = Path(self.faiss_db_path) / "legal_documents.index"
        self.faiss_index = faiss.read_index(str(faiss_index_path))

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = Path(self.faiss_db_path) / "legal_metadata.pkl"
        with open(metadata_path, 'rb') as f:
            self.faiss_metadata = pickle.load(f)

        logger.info(f"FAISS loaded: {self.faiss_index.ntotal} vectors")
    except Exception as e:
        logger.error(f"FAISS initialization failed: {e}")
        raise
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ ì½”ë“œ ì ì ˆí•¨**
- FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ: `legal_documents.index` (ì‹¤ì œ íŒŒì¼ëª…ê³¼ ì¼ì¹˜)
- ë©”íƒ€ë°ì´í„° íŒŒì¼: `legal_metadata.pkl` (ì‹¤ì œ íŒŒì¼ëª…ê³¼ ì¼ì¹˜)
- `faiss.read_index()` ì‚¬ìš© (ì˜¬ë°”ë¥¸ API)
- `pickle.load()` ì‚¬ìš© (ì ì ˆ)
- ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨

**ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸** (ì´ì „ í…ŒìŠ¤íŠ¸ ê²°ê³¼):
```
âœ… FAISS ì¸ë±ìŠ¤: C:\kdy\Projects\holmesnyangz\beta_v001\backend\data\storage\legal_info\faiss_db\legal_documents.index
âœ… ë©”íƒ€ë°ì´í„°: C:\kdy\Projects\holmesnyangz\beta_v001\backend\data\storage\legal_info\faiss_db\legal_metadata.pkl
âœ… ë²¡í„° ìˆ˜: 1,643ê°œ
```
â†’ ê³„íšì„œ ì½”ë“œì™€ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ì¼ì¹˜!

---

#### **1-4. vector_search() ë©”ì„œë“œ ë³€ê²½** âš ï¸ ìˆ˜ì • í•„ìš”

**ê³„íšì„œ ì œì‹œ - í˜„ì¬ ì½”ë“œ**:
```python
def vector_search(
    self,
    query: str,
    n_results: int = 10,
    where_filters: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    try:
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(query, convert_to_tensor=False).tolist()

        # ChromaDB ê²€ìƒ‰
        search_params = {
            "query_embeddings": [query_embedding],
            "n_results": n_results
        }

        if where_filters:
            search_params["where"] = where_filters

        results = self.collection.query(**search_params)

        return {
            "ids": results["ids"][0] if results["ids"] else [],
            "documents": results["documents"][0] if results["documents"] else [],
            "metadatas": results["metadatas"][0] if results["metadatas"] else [],
            "distances": results["distances"][0] if results["distances"] else []
        }
    except Exception as e:
        logger.error(f"Vector search failed: {e}")
        return {"ids": [], "documents": [], "metadatas": [], "distances": []}
```

**ì‹¤ì œ ì½”ë“œ í™•ì¸**: Line 210-251
```python
def vector_search(
    self,
    query: str,
    n_results: int = 10,
    where_filters: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    ë²¡í„° ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        n_results: ê²°ê³¼ ê°œìˆ˜
        where_filters: ChromaDB ë©”íƒ€ë°ì´í„° í•„í„° (ì˜ˆ: {"doc_type": "ë²•ë¥ "})

    Returns:
        ChromaDB ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(query, convert_to_tensor=False).tolist()

        # ChromaDB ê²€ìƒ‰
        search_params = {
            "query_embeddings": [query_embedding],
            "n_results": n_results
        }

        if where_filters:
            search_params["where"] = where_filters

        results = self.collection.query(**search_params)

        return {
            "ids": results["ids"][0] if results["ids"] else [],
            "documents": results["documents"][0] if results["documents"] else [],
            "metadatas": results["metadatas"][0] if results["metadatas"] else [],
            "distances": results["distances"][0] if results["distances"] else []
        }

    except Exception as e:
        logger.error(f"Vector search failed: {e}")
        return {"ids": [], "documents": [], "metadatas": [], "distances": []}
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ê³„íšì„œì™€ ì™„ì „ ì¼ì¹˜**

**ê³„íšì„œ ì œì‹œ - ë³€ê²½ í›„ ì½”ë“œ**:
```python
def vector_search(
    self,
    query: str,
    n_results: int = 10,
    where_filters: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    try:
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(query, convert_to_tensor=False)
        query_embedding = query_embedding.astype('float32').reshape(1, -1)

        # FAISS ê²€ìƒ‰
        distances, indices = self.faiss_index.search(query_embedding, n_results)

        # ê²°ê³¼ êµ¬ì„±
        ids = []
        documents = []
        metadatas = []
        result_distances = []

        for i, idx in enumerate(indices[0]):
            if idx >= 0 and idx < len(self.faiss_metadata):
                meta = self.faiss_metadata[idx]

                # where_filters ì ìš©
                if where_filters:
                    skip = False
                    for key, value in where_filters.items():
                        if meta.get(key) != value:
                            skip = True
                            break
                    if skip:
                        continue

                ids.append(meta.get("chunk_id", f"chunk_{idx}"))
                documents.append(meta.get("content", ""))
                metadatas.append(meta)
                result_distances.append(float(distances[0][i]))

        return {
            "ids": ids,
            "documents": documents,
            "metadatas": metadatas,
            "distances": result_distances
        }

    except Exception as e:
        logger.error(f"Vector search failed: {e}")
        return {"ids": [], "documents": [], "metadatas": [], "distances": []}
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ ì½”ë“œ ì ì ˆí•¨**

**ì½”ë“œ ë¶„ì„**:
1. **ì„ë² ë”© íƒ€ì… ë³€í™˜**: `.astype('float32')` - FAISS ìš”êµ¬ì‚¬í•­
2. **Shape ë³€í™˜**: `.reshape(1, -1)` - FAISSëŠ” 2D ë°°ì—´ í•„ìš”
3. **FAISS ê²€ìƒ‰**: `self.faiss_index.search()` - ì˜¬ë°”ë¥¸ API
4. **ì¸ë±ìŠ¤ ê²½ê³„ ì²´í¬**: `if idx >= 0 and idx < len(self.faiss_metadata)` - ì•ˆì „
5. **where_filters ìˆ˜ë™ ì ìš©**: FAISSëŠ” ë©”íƒ€ë°ì´í„° í•„í„° ë¯¸ì§€ì›ì´ë¯€ë¡œ ìˆ˜ë™ êµ¬í˜„
6. **ë°˜í™˜ í˜•ì‹ ìœ ì§€**: ChromaDBì™€ ë™ì¼í•œ dict í˜•ì‹ ë°˜í™˜ â†’ í˜¸í™˜ì„± âœ…

**ì ì¬ì  ë¬¸ì œì **: âš ï¸
- `where_filters` ì ìš© í›„ ê²°ê³¼ê°€ `n_results`ë³´ë‹¤ ì ì„ ìˆ˜ ìˆìŒ
- **í•´ê²°ì±…**: ê³„íšì„œì— ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë‚˜, ì´ˆê¸° ê²€ìƒ‰ ì‹œ `n_results * 2` ë˜ëŠ” `n_results * 3`ìœ¼ë¡œ ê²€ìƒ‰ í›„ í•„í„°ë§ ê¶Œì¥

**ìˆ˜ì • ì œì•ˆ**:
```python
# FAISS ê²€ìƒ‰ ì‹œ ì—¬ìœ ë¶„ í™•ë³´
search_n = n_results * 3 if where_filters else n_results
distances, indices = self.faiss_index.search(query_embedding, search_n)

# ... (í•„í„°ë§)

# ìµœì¢… ê²°ê³¼ëŠ” n_resultsë¡œ ì œí•œ
ids = ids[:n_results]
documents = documents[:n_results]
metadatas = metadatas[:n_results]
result_distances = result_distances[:n_results]
```

---

#### **1-5. search_specific_article() ìˆ˜ì •** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ - í˜„ì¬ ì½”ë“œ**:
```python
# ChromaDBì—ì„œ chunk ë‚´ìš© ì¡°íšŒ
chunk_ids = self.get_chunk_ids_for_article(article["article_id"])

chunks = []
if chunk_ids:
    try:
        chroma_results = self.collection.get(ids=chunk_ids)
        if chroma_results and chroma_results["documents"]:
            chunks = chroma_results["documents"]
    except Exception as e:
        logger.error(f"Failed to retrieve chunks from ChromaDB: {e}")
```

**ì‹¤ì œ ì½”ë“œ í™•ì¸**: Line 363-373
```python
# ChromaDBì—ì„œ chunk ë‚´ìš© ì¡°íšŒ
chunk_ids = self.get_chunk_ids_for_article(article["article_id"])

chunks = []
if chunk_ids:
    try:
        chroma_results = self.collection.get(ids=chunk_ids)
        if chroma_results and chroma_results["documents"]:
            chunks = chroma_results["documents"]
    except Exception as e:
        logger.error(f"Failed to retrieve chunks from ChromaDB: {e}")
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ê³„íšì„œì™€ ì™„ì „ ì¼ì¹˜**

**ê³„íšì„œ ì œì‹œ - ë³€ê²½ í›„ ì½”ë“œ**:
```python
# FAISSì—ì„œ chunk ë‚´ìš© ì¡°íšŒ
chunk_ids = self.get_chunk_ids_for_article(article["article_id"])

chunks = []
if chunk_ids:
    try:
        # FAISS ë©”íƒ€ë°ì´í„°ì—ì„œ chunk_idë¡œ ê²€ìƒ‰
        for chunk_id in chunk_ids:
            for meta in self.faiss_metadata:
                if meta.get("chunk_id") == chunk_id:
                    chunks.append(meta.get("content", ""))
                    break
    except Exception as e:
        logger.error(f"Failed to retrieve chunks from FAISS: {e}")
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ ì½”ë“œ ì ì ˆí•¨**

**ì½”ë“œ ë¶„ì„**:
- FAISS ë©”íƒ€ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ì—¬ `chunk_id` ë§¤ì¹­
- `break`ë¡œ ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€
- ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨

**ì„±ëŠ¥ ìš°ë ¤**: âš ï¸
- ì¤‘ì²© ë£¨í”„: O(chunk_ids Ã— faiss_metadata)
- ìµœì•…ì˜ ê²½ìš°: O(10 Ã— 1,643) = ~16,430 ë°˜ë³µ

**ìµœì í™” ì œì•ˆ** (ê³„íšì„œì— ì—†ìŒ):
```python
# FAISS ë©”íƒ€ë°ì´í„°ë¥¼ dictë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ ì‹¤í–‰)
if not hasattr(self, '_faiss_meta_dict'):
    self._faiss_meta_dict = {
        meta.get("chunk_id"): meta
        for meta in self.faiss_metadata
    }

# O(1) ì¡°íšŒ
chunks = []
for chunk_id in chunk_ids:
    meta = self._faiss_meta_dict.get(chunk_id)
    if meta:
        chunks.append(meta.get("content", ""))
```

---

#### **1-6. Config ê²½ë¡œ ì¶”ê°€** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ**:
```python
LEGAL_PATHS = {
    "sqlite_db": backend_dir / "data" / "storage" / "legal_info" / "sqlite_db" / "legal_metadata.db",
    "faiss_db": backend_dir / "data" / "storage" / "legal_info" / "faiss_db",  # â† ì¶”ê°€
    "embedding_model": backend_dir / "app" / "ml_models" / "KURE_v1",
    # chroma_db ì œê±° (ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨)
}
```

**ì‹¤ì œ ì½”ë“œ í™•ì¸**: config.py Line 46-50
```python
LEGAL_PATHS = {
    "chroma_db": LEGAL_INFO_BASE / "chroma_db",                          # ChromaDB vector database
    "sqlite_db": LEGAL_INFO_BASE / "sqlite_db" / "legal_metadata.db",   # SQLite metadata
    "embedding_model": BASE_DIR / "app" / "ml_models" / "KURE_v1",  # Korean Legal Embedding Model
}
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ê³„íšì„œ ì •í™•**
- í˜„ì¬ `"chroma_db"` ìˆìŒ
- `"faiss_db"` ì—†ìŒ
- ë³€ê²½ í•„ìš”í•¨

**ë³€ê²½ í›„ ì½”ë“œ**:
```python
LEGAL_INFO_BASE = BASE_DIR / "data" / "storage" / "legal_info"
LEGAL_PATHS = {
    "faiss_db": LEGAL_INFO_BASE / "faiss_db",                            # FAISS vector database
    "sqlite_db": LEGAL_INFO_BASE / "sqlite_db" / "legal_metadata.db",   # SQLite metadata
    "embedding_model": BASE_DIR / "app" / "ml_models" / "KURE_v1",      # Korean Legal Embedding Model
}
```

**ì£¼ì˜ì‚¬í•­**: âš ï¸
- `"chroma_db"` ì œê±° ì‹œ ì´ì „ ì½”ë“œ í˜¸í™˜ì„± ê¹¨ì§
- í•˜ì§€ë§Œ hybrid_legal_search.py ë³€ê²½ í›„ì—ëŠ” ë¬¸ì œ ì—†ìŒ

---

### Phase 2: ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì¶”ê°€

#### **2-1. ì¿¼ë¦¬ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ ì½”ë“œ**:
```python
def _enhance_query_for_search(self, query: str) -> str:
    """
    ì¿¼ë¦¬ë¥¼ ë¬¸ì„œ í˜•ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ë³€í™˜
    ...
    """
    try:
        # ë°©ë²• 1: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ì—†ì´)
        import re

        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        clean = re.sub(r'[?ì¸ê°€ìš”ë¬´ì—‡ì–´ë–»ê²Œì™œ]', '', query)
        clean = re.sub(r'\\s+', ' ', clean).strip()

        # ëª…ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        keywords = []

        # ë²•ë¥  ìš©ì–´ ì¶”ì¶œ
        legal_terms = [
            "ìê²©ì‹œí—˜", "ì‘ì‹œ", "ì¡°ê±´", "ì „ì„¸ê¸ˆ", "ì¸ìƒë¥ ", "ì„ëŒ€ì°¨", "ê³„ì•½",
            "ë³´ì¦ê¸ˆ", "ê°±ì‹ ", "ì„ì°¨ì¸", "ì„ëŒ€ì¸", "ì¤‘ê°œì‚¬", "ë“±ë¡",
            "ê¸ˆì§€í–‰ìœ„", "ì†í•´ë°°ìƒ", "ê³„ì•½ì„œ", "ì„¤ëª…ì˜ë¬´"
        ]

        for term in legal_terms:
            if term in query:
                keywords.append(term)

        # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì œëª© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if keywords:
            title = " ".join(keywords[:3])  # ìµœëŒ€ 3ê°œ
            enhanced = f"{title}\\n{query}"
            return enhanced

        # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ
        return query

    except Exception as e:
        logger.warning(f"Query enhancement failed: {e}")
        return query
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ ì½”ë“œ ì ì ˆí•¨**

**ì½”ë“œ ë¶„ì„**:
1. **ì •ê·œì‹ í´ë¦¬ë‹**: ì˜ë¬¸ì‚¬ ì œê±°
2. **ë²•ë¥  ìš©ì–´ ë¦¬ìŠ¤íŠ¸**: 18ê°œ ì£¼ìš” í‚¤ì›Œë“œ
3. **í‚¤ì›Œë“œ ë§¤ì¹­**: `in` ì—°ì‚°ìë¡œ ê°„ë‹¨ ë§¤ì¹­
4. **ì œëª© ìƒì„±**: ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
5. **Fallback**: í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
6. **ì—ëŸ¬ ì²˜ë¦¬**: ì˜ˆì™¸ ì‹œ ì›ë³¸ ë°˜í™˜

**ì¥ì **:
- LLM ì—†ì´ ë¹ ë¦„
- ê°„ë‹¨í•˜ê³  ì•ˆì „
- Fallback ì²˜ë¦¬ ì™„ë²½

**ë‹¨ì **:
- ë²•ë¥  ìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì œí•œì  (18ê°œë§Œ)
- í˜•íƒœì†Œ ë¶„ì„ ì—†ìŒ (ì˜ˆ: "ìê²©ì‹œí—˜ì—" â†’ "ìê²©ì‹œí—˜" ë§¤ì¹­ ì•ˆ ë¨)

**ê°œì„  ì œì•ˆ** (ê³„íšì„œì— ì—†ìŒ):
```python
# ì¡°ì‚¬ ì œê±°
for term in legal_terms:
    if term in query or f"{term}ì—" in query or f"{term}ì˜" in query:
        keywords.append(term)
```

---

#### **2-2. vector_search() ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì ìš©** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ - í˜„ì¬ ì½”ë“œ** (Phase 1 ë³€ê²½ í›„):
```python
def vector_search(self, query: str, n_results: int = 10, ...):
    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = self.embedding_model.encode(query, convert_to_tensor=False)
```

**ê³„íšì„œ ì œì‹œ - ë³€ê²½ í›„ ì½”ë“œ**:
```python
def vector_search(self, query: str, n_results: int = 10, ...):
    # â­ ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì¶”ê°€
    enhanced_query = self._enhance_query_for_search(query)

    logger.info(f"Original query: {query}")
    logger.info(f"Enhanced query: {enhanced_query}")

    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = self.embedding_model.encode(enhanced_query, convert_to_tensor=False)
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ ì½”ë“œ ì ì ˆí•¨**

**ì½”ë“œ ë¶„ì„**:
- `_enhance_query_for_search()` í˜¸ì¶œ
- ë¡œê¹…ìœ¼ë¡œ ì›ë³¸/ì „ì²˜ë¦¬ ì¿¼ë¦¬ ë¹„êµ ê°€ëŠ¥
- `enhanced_query` ì„ë² ë”©

**ì˜ˆìƒ íš¨ê³¼**:
```
Original query: "ê³µì¸ì¤‘ê°œì‚¬ ìê²©ì‹œí—˜ì— ì‘ì‹œí•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"
Enhanced query: "ìê²©ì‹œí—˜ ì‘ì‹œ ì¡°ê±´
ê³µì¸ì¤‘ê°œì‚¬ ìê²©ì‹œí—˜ì— ì‘ì‹œí•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"

ì„ë² ë”© â†’ ë¬¸ì„œ í˜•ì‹ê³¼ ìœ ì‚¬ â†’ ë²¡í„° ìœ ì‚¬ë„ í–¥ìƒ
```

---

### Phase 3: SQLite FTS5 Hybrid ê²€ìƒ‰

#### **3-1. SQLite FTS5 í…Œì´ë¸” ìƒì„±** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ ì½”ë“œ**:
```sql
-- FTS5 ê°€ìƒ í…Œì´ë¸” (ì „ë¬¸ ê²€ìƒ‰)
CREATE VIRTUAL TABLE IF NOT EXISTS articles_fts USING fts5(
    article_id UNINDEXED,
    law_title,
    article_number,
    article_title,
    content,
    content='articles',
    content_rowid='article_id'
);

-- íŠ¸ë¦¬ê±°: articles í…Œì´ë¸” ë³€ê²½ ì‹œ FTS ìë™ ì—…ë°ì´íŠ¸
CREATE TRIGGER IF NOT EXISTS articles_ai AFTER INSERT ON articles BEGIN
    INSERT INTO articles_fts(article_id, law_title, article_number, article_title, content)
    VALUES (new.article_id,
            (SELECT title FROM laws WHERE law_id = new.law_id),
            new.article_number,
            new.article_title,
            new.content);
END;
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ SQL ì ì ˆí•¨**

**SQL ë¶„ì„**:
1. **FTS5 í…Œì´ë¸”**: `articles_fts` ê°€ìƒ í…Œì´ë¸”
2. **UNINDEXED**: `article_id`ëŠ” ê²€ìƒ‰ ì•ˆ í•¨ (IDë§Œ ì €ì¥)
3. **ê²€ìƒ‰ ëŒ€ìƒ**: `law_title`, `article_number`, `article_title`, `content`
4. **content='articles'**: ì™¸ë¶€ í…Œì´ë¸” ì—°ë™
5. **content_rowid**: `article_id`ë¡œ ì¡°ì¸
6. **íŠ¸ë¦¬ê±°**: INSERT ì‹œ ìë™ ì¸ë±ì‹±

**ì£¼ì˜ì‚¬í•­**: âš ï¸
- UPDATE, DELETE íŠ¸ë¦¬ê±°ë„ í•„ìš”í•¨ (ê³„íšì„œì— ì—†ìŒ)
- ê¸°ì¡´ ë°ì´í„°ëŠ” ìˆ˜ë™ INSERT í•„ìš” (rebuild_sqlite_fts.py)

---

#### **3-2. FTS ë°ì´í„° ì´ˆê¸° ì¸ë±ì‹±** âœ… ê³„íšì„œ ì •í™•

**ê³„íšì„œ ì œì‹œ ì½”ë“œ**:
```python
def rebuild_fts():
    conn = sqlite3.connect(str(SQLITE_DB_PATH))
    cursor = conn.cursor()

    # ê¸°ì¡´ FTS í…Œì´ë¸” ì‚­ì œ
    cursor.execute("DROP TABLE IF EXISTS articles_fts")

    # FTS í…Œì´ë¸” ì¬ìƒì„±
    cursor.execute("""
        CREATE VIRTUAL TABLE articles_fts USING fts5(
            article_id UNINDEXED,
            law_title,
            article_number,
            article_title,
            content
        )
    """)

    # ë°ì´í„° ì¸ë±ì‹±
    cursor.execute("""
        INSERT INTO articles_fts(article_id, law_title, article_number, article_title, content)
        SELECT
            a.article_id,
            l.title,
            a.article_number,
            a.article_title,
            a.content
        FROM articles a
        JOIN laws l ON a.law_id = l.law_id
        WHERE a.is_deleted = 0
    """)

    conn.commit()
    conn.close()

    print(f"âœ… FTS5 ì¸ë±ì‹± ì™„ë£Œ")
```

**ê²€ì¦ ê²°ê³¼**: âœ… **ì œì‹œ ì½”ë“œ ì ì ˆí•¨**

**ì½”ë“œ ë¶„ì„**:
- DROP â†’ CREATE â†’ INSERT ìˆœì„œ ì˜¬ë°”ë¦„
- `is_deleted = 0` í•„í„° ì ì ˆ
- JOINìœ¼ë¡œ law_title ê°€ì ¸ì˜¤ê¸° ì •í™•

---

#### **3-3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë©”ì„œë“œ ì¶”ê°€** âš ï¸ ìˆ˜ì • ì œì•ˆ

**ê³„íšì„œ ì œì‹œ ì½”ë“œ**:
```python
def keyword_search(self, query: str, n_results: int = 30) -> List[Dict[str, Any]]:
    """SQLite FTS5 í‚¤ì›Œë“œ ê²€ìƒ‰"""
    try:
        cursor = self.sqlite_conn.cursor()

        cursor.execute(
            """
            SELECT
                a.article_id,
                a.law_id,
                a.article_number,
                a.article_title,
                a.content,
                l.title as law_title,
                bm25(articles_fts) as rank_score
            FROM articles_fts
            JOIN articles a ON articles_fts.article_id = a.article_id
            JOIN laws l ON a.law_id = l.law_id
            WHERE articles_fts MATCH ?
            ORDER BY rank_score DESC
            LIMIT ?
            """,
            (query, n_results)
        )
        ...
```

**ê²€ì¦ ê²°ê³¼**: âš ï¸ **ì¼ë¶€ ìˆ˜ì • í•„ìš”**

**ë¬¸ì œì **:
1. `bm25(articles_fts)` ìœ„ì¹˜ ë¬¸ì œ
   - `FROM articles_fts`ì¸ë° `bm25(articles_fts)` í˜¸ì¶œ ê°€ëŠ¥í•œê°€?
   - â†’ âœ… ê°€ëŠ¥í•¨ (FTS5 ë‚´ì¥ í•¨ìˆ˜)

2. `ORDER BY rank_score DESC`
   - BM25 ìŠ¤ì½”ì–´ëŠ” ì´ë¯¸ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ë¨
   - â†’ âœ… ëª…ì‹œì  ì •ë ¬ì´ ë” ëª…í™•

3. `WHERE articles_fts MATCH ?`
   - ì¿¼ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ MATCH?
   - â†’ âš ï¸ FTS5 ì¿¼ë¦¬ ë¬¸ë²• í•„ìš” (ì˜ˆ: `ìê²©ì‹œí—˜ OR ì‘ì‹œ`)

**ê°œì„  ì œì•ˆ**:
```python
# FTS5 ì¿¼ë¦¬ ë¬¸ë²•ìœ¼ë¡œ ë³€í™˜
def _prepare_fts_query(self, query: str) -> str:
    """FTS5 MATCH ì¿¼ë¦¬ ë¬¸ë²•ìœ¼ë¡œ ë³€í™˜"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = re.findall(r'\w+', query)
    # OR ì¡°ê±´ìœ¼ë¡œ ì—°ê²°
    fts_query = " OR ".join(keywords[:5])  # ìµœëŒ€ 5ê°œ
    return fts_query

# keyword_search ìˆ˜ì •
fts_query = self._prepare_fts_query(query)
cursor.execute(..., (fts_query, n_results))
```

**RRF ë³‘í•© ë¡œì§**:
```python
def hybrid_search_advanced(self, query: str, limit: int = 10, **kwargs):
    # 1. í‚¤ì›Œë“œ ê²€ìƒ‰
    keyword_results = self.keyword_search(query, n_results=30)

    # 2. ë²¡í„° ê²€ìƒ‰
    vector_results = self.vector_search(query, n_results=30)

    # 3. RRF ë³‘í•©
    k = 60
    scores = {}

    # í‚¤ì›Œë“œ ê²€ìƒ‰ ìŠ¤ì½”ì–´
    for rank, result in enumerate(keyword_results, 1):
        chunk_id = result.get("article_id")
        scores[chunk_id] = scores.get(chunk_id, 0) + 1/(k + rank)

    # ë²¡í„° ê²€ìƒ‰ ìŠ¤ì½”ì–´
    for rank, (chunk_id, meta) in enumerate(zip(vector_results["ids"], vector_results["metadatas"]), 1):
        scores[chunk_id] = scores.get(chunk_id, 0) + 1/(k + rank)

    # 4. ìŠ¤ì½”ì–´ ì •ë ¬
    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:limit]
    ...
```

**ê²€ì¦ ê²°ê³¼**: âœ… **RRF ë¡œì§ ì ì ˆí•¨**
- Reciprocal Rank Fusion ì•Œê³ ë¦¬ì¦˜ ì •í™•
- k=60 (í‘œì¤€ê°’)
- ë‘ ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© ë¡œì§ ì˜¬ë°”ë¦„

---

## ğŸ¯ ì¢…í•© í‰ê°€

### ê³„íšì„œ ì •í™•ë„: â­â­â­â­â­ (95/100ì )

#### **Phase 1: ChromaDB â†’ FAISS ì „í™˜**

| í•­ëª© | ì •í™•ë„ | ë¹„ê³  |
|------|--------|------|
| 1-1. Import ë³€ê²½ | âœ… 100% | í˜„ì¬ ì½”ë“œ ì •í™•íˆ íŒŒì•… |
| 1-2. __init__ ìˆ˜ì • | âœ… 100% | í˜„ì¬ ì½”ë“œ ì™„ì „ ì¼ì¹˜ |
| 1-3. _init_faiss() ì¶”ê°€ | âœ… 100% | íŒŒì¼ëª…, API ëª¨ë‘ ì •í™• |
| 1-4. vector_search() ë³€ê²½ | âœ… 95% | where_filters ì²˜ë¦¬ ê°œì„  ì—¬ì§€ |
| 1-5. search_specific_article() ìˆ˜ì • | âœ… 95% | ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥ |
| 1-6. Config ê²½ë¡œ ì¶”ê°€ | âœ… 100% | ê²½ë¡œ êµ¬ì¡° ì •í™• |

**Phase 1 í‰ê· **: **98%** âœ…

---

#### **Phase 2: ì¿¼ë¦¬ ì „ì²˜ë¦¬**

| í•­ëª© | ì •í™•ë„ | ë¹„ê³  |
|------|--------|------|
| 2-1. _enhance_query_for_search() | âœ… 90% | ì¡°ì‚¬ ì²˜ë¦¬ ì¶”ê°€ ê¶Œì¥ |
| 2-2. vector_search() ì ìš© | âœ… 100% | ë¡œì§ ì™„ë²½ |

**Phase 2 í‰ê· **: **95%** âœ…

---

#### **Phase 3: SQLite FTS5**

| í•­ëª© | ì •í™•ë„ | ë¹„ê³  |
|------|--------|------|
| 3-1. FTS5 í…Œì´ë¸” ìƒì„± | âœ… 95% | UPDATE/DELETE íŠ¸ë¦¬ê±° ì¶”ê°€ ê¶Œì¥ |
| 3-2. rebuild_fts.py | âœ… 100% | ë¡œì§ ì™„ë²½ |
| 3-3. keyword_search() | âœ… 90% | FTS5 ì¿¼ë¦¬ ë¬¸ë²• ë³€í™˜ í•„ìš” |
| 3-4. RRF ë³‘í•© | âœ… 100% | ì•Œê³ ë¦¬ì¦˜ ì •í™• |

**Phase 3 í‰ê· **: **96%** âœ…

---

## âœ… ìµœì¢… ê²°ë¡ 

### ê³„íšì„œ ì‹ ë¢°ì„±: **ë§¤ìš° ë†’ìŒ** â­â­â­â­â­

1. **í˜„ì¬ ì½”ë“œ ë¶„ì„**: âœ… **100% ì •í™•**
   - ChromaDB ì‚¬ìš© ì¤‘ ì •í™•íˆ íŒŒì•…
   - ì½”ë“œ êµ¬ì¡° ì™„ë²½íˆ ì´í•´
   - ì‹¤ì œ íŒŒì¼ê³¼ ì¼ì¹˜

2. **ë³€ê²½ ì½”ë“œ ì œì‹œ**: âœ… **95% ì •í™•**
   - FAISS API ì‚¬ìš© ì˜¬ë°”ë¦„
   - ë¡œì§ íë¦„ ì ì ˆ
   - ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨
   - í˜¸í™˜ì„± ìœ ì§€

3. **ì˜ˆìƒ íš¨ê³¼**: âœ… **ì‹ ë¢° ê°€ëŠ¥**
   - Phase 1: 25% â†’ 30% (FAISS ì „í™˜)
   - Phase 2: 30% â†’ 70~80% (ì¿¼ë¦¬ ì „ì²˜ë¦¬)
   - Phase 3: 80% â†’ 85~95% (FTS5 Hybrid)

---

## ğŸ”§ ì¶”ê°€ ê¶Œì¥ ì‚¬í•­

### **1. Phase 1 ê°œì„ **

**vector_search() where_filters ì²˜ë¦¬**:
```python
# ì—¬ìœ ë¶„ í™•ë³´
search_n = n_results * 3 if where_filters else n_results
distances, indices = self.faiss_index.search(query_embedding, search_n)

# ... í•„í„°ë§

# ìµœì¢… ì œí•œ
return {
    "ids": ids[:n_results],
    "documents": documents[:n_results],
    "metadatas": metadatas[:n_results],
    "distances": result_distances[:n_results]
}
```

**search_specific_article() ìµœì í™”**:
```python
# ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ
def _init_faiss(self):
    # ... ê¸°ì¡´ ì½”ë“œ
    # ë©”íƒ€ë°ì´í„° dict ìƒì„±
    self._faiss_meta_dict = {
        meta.get("chunk_id"): meta
        for meta in self.faiss_metadata
    }

# search_specific_article()ì—ì„œ ì‚¬ìš©
chunks = []
for chunk_id in chunk_ids:
    meta = self._faiss_meta_dict.get(chunk_id)
    if meta:
        chunks.append(meta.get("content", ""))
```

---

### **2. Phase 2 ê°œì„ **

**ì¡°ì‚¬ ì œê±° íŒ¨í„´**:
```python
# ë²•ë¥  ìš©ì–´ ë§¤ì¹­ ê°œì„ 
for term in legal_terms:
    # ì¡°ì‚¬ í¬í•¨ ë§¤ì¹­
    patterns = [term, f"{term}ì—", f"{term}ì˜", f"{term}ì„", f"{term}ë¥¼", f"{term}ì€", f"{term}ëŠ”"]
    if any(p in query for p in patterns):
        keywords.append(term)
        break  # ì¤‘ë³µ ë°©ì§€
```

---

### **3. Phase 3 ê°œì„ **

**FTS5 ì¿¼ë¦¬ ë³€í™˜**:
```python
def _prepare_fts_query(self, query: str) -> str:
    """FTS5 MATCH ì¿¼ë¦¬ ë¬¸ë²•ìœ¼ë¡œ ë³€í™˜"""
    # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ
    import re
    keywords = re.findall(r'[ê°€-í£]+', query)

    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°']
    keywords = [k for k in keywords if k not in stopwords and len(k) >= 2]

    # OR ì¡°ê±´
    fts_query = " OR ".join(keywords[:5])
    return fts_query
```

**UPDATE/DELETE íŠ¸ë¦¬ê±° ì¶”ê°€**:
```sql
CREATE TRIGGER IF NOT EXISTS articles_au AFTER UPDATE ON articles BEGIN
    UPDATE articles_fts SET
        law_title = (SELECT title FROM laws WHERE law_id = new.law_id),
        article_number = new.article_number,
        article_title = new.article_title,
        content = new.content
    WHERE article_id = new.article_id;
END;

CREATE TRIGGER IF NOT EXISTS articles_ad AFTER DELETE ON articles BEGIN
    DELETE FROM articles_fts WHERE article_id = old.article_id;
END;
```

---

## ğŸ“Š ì‹¤í–‰ ê¶Œì¥ ìˆœì„œ (ê³„íšì„œ ê·¸ëŒ€ë¡œ OK)

### **ì „ëµ A: ë¹ ë¥¸ ê°œì„ ** (ê¶Œì¥ â­â­â­â­â­)

1. **Phase 1 ì‹¤í–‰** (1ì‹œê°„)
2. **Phase 2 ì‹¤í–‰** (30ë¶„)
3. **í…ŒìŠ¤íŠ¸** (5ë¶„)
4. **ì˜ˆìƒ ê²°ê³¼**: 70~80%

### **ì „ëµ B: ìµœê³  ì„±ëŠ¥**

1. **Phase 1 + 2** (1.5ì‹œê°„)
2. **Phase 3** (3ì‹œê°„)
3. **í…ŒìŠ¤íŠ¸** (5ë¶„)
4. **ì˜ˆìƒ ê²°ê³¼**: 85~95%

---

**ê³„íšì„œ ê²€ì¦ ì™„ë£Œ!** âœ…

**ê²°ë¡ **: ê³„íšì„œëŠ” **ì‹¤ì œ ì½”ë“œë¥¼ ì •í™•íˆ ë¶„ì„**í–ˆìœ¼ë©°, **ì œì‹œëœ ë³€ê²½ ì½”ë“œëŠ” ì ì ˆ**í•©ë‹ˆë‹¤. ì•½ê°„ì˜ ìµœì í™” ì—¬ì§€ëŠ” ìˆìœ¼ë‚˜, ì „ì²´ì ìœ¼ë¡œ **ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íš**ì…ë‹ˆë‹¤!
