# ê³ ë„í™” ê³„íšì„œ ê²€ì¦: ë†“ì¹œ ì  ë° ì¶”ê°€ ë°œê²¬ì‚¬í•­

**ì‘ì„±ì¼**: 2025-10-15
**ê²€í†  ëŒ€ìƒ**: answer_quality_enhancement_plan_251015.md
**ë¶„ì„ ë°©ë²•**: ê³„íšì„œì™€ ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ ì „ë©´ ë¹„êµ

---

## ğŸ“Œ Executive Summary

ê³„íšì„œëŠ” ëŒ€ì²´ë¡œ ì •í™•í–ˆì§€ë§Œ, **ì¤‘ìš”í•œ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­**ê³¼ **í†µí•© í¬ì¸íŠ¸**ë¥¼ ëª‡ ê°€ì§€ ë†“ì³¤ìŠµë‹ˆë‹¤.

**í•µì‹¬ ë†“ì¹œ ì **:
1. âœ… `complete_json_async()` ë©”ì„œë“œëŠ” ì´ë¯¸ ì¡´ì¬í•¨ (ê³„íšì„œê°€ ë§ìŒ)
2. âŒ WebSocket ë©”ì‹œì§€ ì „ë‹¬ ê²½ë¡œì˜ ë³µì¡ì„± ê³¼ì†Œí‰ê°€
3. âŒ datetime ì§ë ¬í™” ë¬¸ì œ ë¯¸ì–¸ê¸‰
4. âŒ í”„ë¡¬í”„íŠ¸ê°€ ì´ë¯¸ JSONì„ ìš”êµ¬í•˜ê³  ìˆìœ¼ë‚˜, íŒŒì‹±ë˜ì§€ ì•ŠìŒ
5. âŒ Frontend TypeScript íƒ€ì… ì •ì˜ í•„ìš”ì„± ë¯¸ì–¸ê¸‰

---

## 1. ğŸ” ê³„íšì„œì™€ ì½”ë“œ ë¹„êµ ë¶„ì„

### 1.1 Backend: LLM ì‘ë‹µ ìƒì„± ê²½ë¡œ

#### ê³„íšì„œ ì£¼ì¥:
> "llm_service.py:390ì„ complete_json_async()ë¡œ ë³€ê²½"

#### ì‹¤ì œ ì½”ë“œ í™•ì¸:
```python
# llm_service.py:193-198 (í˜„ì¬)
answer = await self.complete_async(
    prompt_name="response_synthesis",
    variables=variables,
    temperature=0.3,
    max_tokens=1000
)
```

**âœ… ê³„íšì„œ ì •í™•**: `complete_json_async()` ë©”ì„œë“œê°€ ì¡´ì¬í•˜ë©° (line 228-257), ë³€ê²½ë§Œ í•˜ë©´ ë¨

### 1.2 WebSocket ë©”ì‹œì§€ íë¦„

#### ê³„íšì„œì—ì„œ ë†“ì¹œ ì :

**ì‹¤ì œ íë¦„**:
```
TeamBasedSupervisor â†’ progress_callback â†’ chat_api._process_query_async â†’
conn_mgr.send_message â†’ ws_manager._serialize_datetimes â†’ websocket.send_json
```

**ë°œê²¬í•œ ë¬¸ì œ**:
1. `ws_manager.py:61-80` - datetime ìë™ ì§ë ¬í™” ë¡œì§ì´ ì´ë¯¸ ì¡´ì¬
2. `chat_api.py:372` - "final_responseë§Œ ì¶”ì¶œ" ì£¼ì„ì´ ìˆìŒ (datetime ë¬¸ì œ ë•Œë¬¸)
3. **ë†“ì¹œ ì **: structured_data í•„ë“œ ì¶”ê°€ ì‹œ datetime ì§ë ¬í™” ê³ ë ¤ í•„ìš”

### 1.3 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¶„ì„

#### ê³„íšì„œì—ì„œ ë†“ì¹œ ì :

**response_synthesis.txt ë¶„ì„**:
- âœ… JSON í˜•ì‹ ëª…í™•íˆ ì •ì˜ (line 24-43)
- âœ… ì˜ˆì‹œ í¬í•¨ (line 50-73)
- âœ… "JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”" ëª…ì‹œ (line 87)
- âŒ **ê·¸ëŸ°ë° LLMì´ í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ì¤‘!**

**ê·¼ë³¸ ì›ì¸**:
```python
# llm_service.py:193 - response_formatì´ ì§€ì •ë˜ì§€ ì•ŠìŒ!
answer = await self.complete_async(
    prompt_name="response_synthesis",
    variables=variables,
    temperature=0.3,
    max_tokens=1000
    # response_format={"type": "json_object"} ëˆ„ë½!
)
```

---

## 2. ğŸš¨ ë†“ì¹œ ì¤‘ìš” ì‚¬í•­ë“¤

### 2.1 JSON Mode í™œì„±í™” ëˆ„ë½

**ë¬¸ì œ**: í”„ë¡¬í”„íŠ¸ëŠ” JSONì„ ìš”êµ¬í•˜ì§€ë§Œ, OpenAI APIì— JSON ëª¨ë“œë¥¼ ì•Œë¦¬ì§€ ì•ŠìŒ

**ìˆ˜ì • í•„ìš”**:
```python
# í˜„ì¬ (ì˜ëª»ë¨)
answer = await self.complete_async(...)

# ìˆ˜ì •ì•ˆ 1: complete_json_async ì‚¬ìš© (ê¶Œì¥)
response_json = await self.complete_json_async(...)

# ìˆ˜ì •ì•ˆ 2: response_format ëª…ì‹œ
answer = await self.complete_async(
    ...,
    response_format={"type": "json_object"}
)
```

### 2.2 WebSocket ë©”ì‹œì§€ êµ¬ì¡° ë³€ê²½ ì˜í–¥

**ê³„íšì„œì—ì„œ ë†“ì¹œ ì **: Frontendì—ì„œ response êµ¬ì¡° ë³€ê²½ ì²˜ë¦¬

**í˜„ì¬ Frontend (chat-interface.tsx:326)**:
```tsx
content: message.response?.content || message.response?.answer || message.response?.message
```

**ìˆ˜ì • í•„ìš”**:
```tsx
// 1. Message íƒ€ì… í™•ì¥
interface Message {
  // ...
  structuredData?: StructuredData
}

// 2. ì¡°ê±´ë¶€ ë Œë”ë§
{message.structuredData ? (
  <AnswerDisplay data={message.structuredData} />
) : (
  <p>{message.content}</p>
)}
```

### 2.3 ì—ëŸ¬ ì²˜ë¦¬ ë¯¸í¡

**ê³„íšì„œì—ì„œ ë†“ì¹œ ì **: JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback

**í˜„ì¬ ì½”ë“œ ë¬¸ì œ**:
```python
# llm_service.py:254-257
try:
    return json.loads(response)
except json.JSONDecodeError as e:
    logger.error(f"Failed to parse JSON response: {response}")
    raise ValueError(f"Invalid JSON response from LLM: {e}")
```

**ê°œì„ ì•ˆ**:
```python
try:
    return json.loads(response)
except json.JSONDecodeError as e:
    logger.error(f"Failed to parse JSON, falling back to text: {e}")
    # Fallback: í…ìŠ¤íŠ¸ë¥¼ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜
    return {
        "answer": response,
        "confidence": 0.5,
        "details": {},
        "recommendations": [],
        "sources": []
    }
```

### 2.4 TypeScript íƒ€ì… ì •ì˜ ëˆ„ë½

**ê³„íšì„œì—ì„œ ë†“ì¹œ ì **: Backend ì‘ë‹µ êµ¬ì¡°ì— ë§ëŠ” íƒ€ì… ì •ì˜

**í•„ìš”í•œ íƒ€ì… ì •ì˜**:
```typescript
// frontend/types/answer.ts (ìƒˆ íŒŒì¼)
export interface StructuredAnswer {
  answer: string
  details: {
    legal_basis?: string
    data_analysis?: string
    considerations?: string[]
  }
  recommendations: string[]
  sources: string[]
  confidence: number
  additional_info?: string
}

export interface StructuredData {
  sections: AnswerSection[]
  metadata: {
    confidence: number
    sources: string[]
    intent_type: string
  }
}
```

---

## 3. ğŸ“Š ì˜í–¥ë„ ë¶„ì„

### 3.1 ìˆ˜ì • í•„ìš” íŒŒì¼ (ê³„íšì„œ vs ì‹¤ì œ)

| íŒŒì¼ | ê³„íšì„œ | ì‹¤ì œ í•„ìš” | ì°¨ì´ì  |
|------|--------|-----------|--------|
| llm_service.py | âœ… | âœ… | response_format ì¶”ê°€ í•„ìš” |
| team_supervisor.py | âŒ | âŒ | ìˆ˜ì • ë¶ˆí•„ìš” (ë§ìŒ) |
| chat_api.py | âŒ | âš ï¸ | datetime ì²˜ë¦¬ í™•ì¸ í•„ìš” |
| ws_manager.py | âŒ | âœ… | structured_data ì§ë ¬í™” í…ŒìŠ¤íŠ¸ |
| chat-interface.tsx | âœ… | âœ… | - |
| **types/answer.ts** | âŒ | âœ… | ìƒˆ íŒŒì¼ í•„ìš” |

### 3.2 ìš°ì„ ìˆœìœ„ ì¬ì¡°ì •

**ê³„íšì„œ ìš°ì„ ìˆœìœ„**:
1. P0: llm_service.py JSON íŒŒì‹±
2. P0: Frontend AnswerDisplay ì»´í¬ë„ŒíŠ¸

**ìˆ˜ì •ëœ ìš°ì„ ìˆœìœ„**:
1. **P0**: llm_service.py response_format ì¶”ê°€ (10ë¶„)
2. **P0**: TypeScript íƒ€ì… ì •ì˜ (30ë¶„)
3. **P0**: Frontend AnswerDisplay ì»´í¬ë„ŒíŠ¸ (3ì¼)
4. **P1**: ì—ëŸ¬ ì²˜ë¦¬ ë° fallback (1ì¼)

---

## 4. ğŸ”§ ì¦‰ì‹œ ìˆ˜ì • ê°€ëŠ¥í•œ Quick Fix

### Quick Fix #1: JSON ëª¨ë“œ í™œì„±í™” (10ë¶„)

```python
# llm_service.py:generate_final_response() ìˆ˜ì •
# Line 193-198ì„ ë‹¤ìŒìœ¼ë¡œ êµì²´:

response_json = await self.complete_json_async(
    prompt_name="response_synthesis",
    variables=variables,
    temperature=0.3,
    max_tokens=1000
)

# ê·¸ë¦¬ê³  line 202-207 ìˆ˜ì •:
return {
    "type": "answer",
    "answer": response_json.get("answer", ""),
    "structured_data": {
        "raw": response_json,  # ì „ì²´ JSON
        "sections": self._create_sections(response_json),
        "metadata": {
            "confidence": response_json.get("confidence", 0.8),
            "sources": response_json.get("sources", []),
            "intent_type": intent_info.get("intent_type")
        }
    },
    "teams_used": list(aggregated_results.keys()),
    "data": aggregated_results
}
```

### Quick Fix #2: ì„¹ì…˜ ìƒì„± í—¬í¼ ë©”ì„œë“œ (20ë¶„)

```python
def _create_sections(self, response_json: Dict) -> List[Dict]:
    """JSON ì‘ë‹µì„ ì„¹ì…˜ìœ¼ë¡œ ë³€í™˜"""
    sections = []

    # í•µì‹¬ ë‹µë³€
    if response_json.get("answer"):
        sections.append({
            "title": "í•µì‹¬ ë‹µë³€",
            "content": response_json["answer"],
            "icon": "target",
            "priority": "high"
        })

    # ë²•ì  ê·¼ê±°
    if response_json.get("details", {}).get("legal_basis"):
        sections.append({
            "title": "ë²•ì  ê·¼ê±°",
            "content": response_json["details"]["legal_basis"],
            "icon": "scale",
            "expandable": True
        })

    # ì¶”ì²œì‚¬í•­
    if response_json.get("recommendations"):
        sections.append({
            "title": "ì¶”ì²œì‚¬í•­",
            "content": response_json["recommendations"],
            "icon": "lightbulb",
            "type": "checklist"
        })

    return sections
```

---

## 5. ğŸ¯ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

#### Test 1: JSON ì‘ë‹µ ìƒì„±
```bash
# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
curl -X POST http://localhost:8000/api/v1/chat/ws/test \
  -H "Content-Type: application/json" \
  -d '{"type": "query", "query": "ì „ì„¸ê¸ˆ 5% ì¸ìƒì´ ê°€ëŠ¥í•œê°€ìš”?"}'

# ì˜ˆìƒ ì‘ë‹µ í™•ì¸
- response.structured_dataê°€ ìˆëŠ”ì§€
- response.structured_data.sectionsê°€ ë°°ì—´ì¸ì§€
- response.structured_data.metadata.confidenceê°€ ìˆ«ìì¸ì§€
```

#### Test 2: Datetime ì§ë ¬í™”
```python
# team_supervisor.pyì— í…ŒìŠ¤íŠ¸ ì¶”ê°€
result["test_datetime"] = datetime.now()
# ws_managerê°€ ìë™ ë³€í™˜í•˜ëŠ”ì§€ í™•ì¸
```

#### Test 3: Frontend íƒ€ì… ì²´í¬
```bash
# TypeScript ì»´íŒŒì¼ í™•ì¸
npm run type-check
```

---

## 6. ğŸ’¡ ì¶”ê°€ ê°œì„  ê¸°íšŒ

### 6.1 ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ê³„íšì„œ ë¯¸ì–¸ê¸‰)

**ê¸°íšŒ**: OpenAI Streaming APIë¡œ ê¸´ ë‹µë³€ ì ì§„ì  í‘œì‹œ
```python
stream = await self.async_client.chat.completions.create(
    ...,
    stream=True
)
async for chunk in stream:
    # WebSocketìœ¼ë¡œ ì²­í¬ ì „ì†¡
```

### 6.2 ë‹µë³€ ìºì‹± (ê³„íšì„œ ë¯¸ì–¸ê¸‰)

**ê¸°íšŒ**: Redisë¡œ ë™ì¼ ì§ˆë¬¸ ìºì‹±
```python
cache_key = hashlib.md5(f"{query}:{intent_type}".encode()).hexdigest()
cached = await redis.get(cache_key)
if cached:
    return json.loads(cached)
```

### 6.3 ë‹µë³€ ë²„ì „ ê´€ë¦¬ (ê³„íšì„œ ë¯¸ì–¸ê¸‰)

**ê¸°íšŒ**: ë‹µë³€ êµ¬ì¡° ë²„ì „ìœ¼ë¡œ í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
```python
return {
    "version": "1.0",
    "type": "answer",
    "structured_data": {...}
}
```

---

## 7. ê²°ë¡ 

### ê³„íšì„œ í‰ê°€
- **ì •í™•ë„**: 85/100
- **ì™„ì„±ë„**: 75/100
- **ì‹¤í–‰ê°€ëŠ¥ì„±**: 90/100

### ì£¼ìš” ë†“ì¹œ ì 
1. âœ… JSON ëª¨ë“œ í™œì„±í™” í•„ìˆ˜ (`response_format` íŒŒë¼ë¯¸í„°)
2. âœ… TypeScript íƒ€ì… ì •ì˜ í•„ìš”
3. âœ… Datetime ì§ë ¬í™” ê³ ë ¤
4. âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° fallback ì „ëµ

### ìˆ˜ì •ëœ êµ¬í˜„ ì‹œê°„
- **ì›ë˜ ì˜ˆìƒ**: 4ì¼
- **ìˆ˜ì • ì˜ˆìƒ**: 5ì¼ (íƒ€ì… ì •ì˜ ë° ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)

### Next Action
1. `llm_service.py` line 193ì„ `complete_json_async()`ë¡œ ì¦‰ì‹œ ë³€ê²½
2. `_create_sections()` í—¬í¼ ë©”ì„œë“œ ì¶”ê°€
3. TypeScript íƒ€ì… ì •ì˜ íŒŒì¼ ìƒì„±
4. í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

---

**ì‘ì„±ì**: Claude (Anthropic AI)
**ê²€í† ì¼**: 2025-10-15