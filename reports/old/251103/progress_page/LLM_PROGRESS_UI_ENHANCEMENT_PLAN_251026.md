# LLM ì‘ë‹µ ìƒì„± ì¤‘ ì§„í–‰ ìƒíƒœ UI ê°œì„  ê³„íšì„œ

**ì‘ì„±ì¼**: 2025-10-26
**ëª©ì **: LLM ì‘ë‹µ ìƒì„± ì‹œ ë©ˆì¶¤ í˜„ìƒ ê°œì„  - ì¤‘ê°„ ì§„í–‰ ìƒíƒœ í‘œì‹œ ì¶”ê°€
**ìš°ì„ ìˆœìœ„**: P1 (ì‚¬ìš©ì ê²½í—˜ ê°œì„  í•„ìˆ˜)

---

## ğŸ“‹ ëª©ì°¨

1. [ë¬¸ì œ ë¶„ì„](#ë¬¸ì œ-ë¶„ì„)
2. [í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„](#í˜„ì¬-ì‹œìŠ¤í…œ-ë¶„ì„)
3. [ê°œì„  ë°©ì•ˆ](#ê°œì„ -ë°©ì•ˆ)
4. [êµ¬í˜„ ê³„íš](#êµ¬í˜„-ê³„íš)
5. [ì˜ˆìƒ íš¨ê³¼](#ì˜ˆìƒ-íš¨ê³¼)

---

## ë¬¸ì œ ë¶„ì„

### ë¡œê·¸ ë¶„ì„

```
2025-10-26 14:10:54 - [TeamSupervisor] Using LLM for response generation
2025-10-26 14:11:03 - LLM Call: response_synthesis | Tokens: 1431
                      ^^^^^^^^^ 9ì´ˆ ì†Œìš”
2025-10-26 14:11:03 - [TeamSupervisor] Saving conversation to Long-term Memory
2025-10-26 14:11:03 - Background summary task created
2025-10-26 14:11:06 - LLM Call: conversation_summary | Tokens: 336
                      ^^^^^^^^^ 3ì´ˆ ì†Œìš”
2025-10-26 14:11:06 - Summary saved
```

### íƒ€ì„ë¼ì¸

```
Phase               | ì‹œì‘       | ì¢…ë£Œ       | ì†Œìš” ì‹œê°„ | UI ìƒíƒœ
--------------------|-----------|-----------|----------|---------------------------
Query ìˆ˜ì‹           | 14:10:43  | 14:10:54  | 11ì´ˆ     | "ë¶„ì„ ì¤‘" â†’ "ì‹¤í–‰ ì¤‘"
LLM ì‘ë‹µ ìƒì„±       | 14:10:54  | 14:11:03  | 9ì´ˆ      | "ë‹µë³€ ì‘ì„± ì¤‘" (ë³€í™” ì—†ìŒ) âŒ
Memory ì €ì¥         | 14:11:03  | 14:11:03  | <1ì´ˆ     | (ë³€í™” ì—†ìŒ) âŒ
Summary ìƒì„±        | 14:11:03  | 14:11:06  | 3ì´ˆ      | (ë³€í™” ì—†ìŒ) âŒ
ìµœì¢… ì‘ë‹µ           | 14:11:06  | -         | -        | ì™„ë£Œ
```

**ì´ ëŒ€ê¸° ì‹œê°„**: 9ì´ˆ (LLM) + 3ì´ˆ (Summary) = **12ì´ˆê°„ UI ë³€í™” ì—†ìŒ**

### ì‚¬ìš©ì ê²½í—˜ ë¬¸ì œ

1. **ë©ˆì¶¤ í˜„ìƒ**: 12ì´ˆê°„ í™”ë©´ì— ë³€í™”ê°€ ì—†ì–´ ë¸Œë¼ìš°ì €ê°€ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ëŠê»´ì§
2. **ë¶ˆì•ˆê°**: ì²˜ë¦¬ê°€ ì§„í–‰ë˜ëŠ”ì§€, ì˜¤ë¥˜ê°€ ë°œìƒí–ˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŒ
3. **ì´íƒˆ ìœ„í—˜**: ì‚¬ìš©ìê°€ ìƒˆë¡œê³ ì¹¨í•˜ê±°ë‚˜ í˜ì´ì§€ë¥¼ ë– ë‚  ê°€ëŠ¥ì„± ì¦ê°€

### ì‹¬ê°ë„

- **UX ì˜í–¥ë„**: ğŸ”´ High (ì‚¬ìš©ì ì´íƒˆ ê°€ëŠ¥)
- **ê¸°ìˆ ì  ë‚œì´ë„**: ğŸŸ¡ Medium (WebSocket ë©”ì‹œì§€ ì¶”ê°€)
- **ìš°ì„ ìˆœìœ„**: **P1** (ì¦‰ì‹œ ê°œì„  í•„ìš”)

---

## í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### 1. Backend Progress Callback êµ¬ì¡°

**íŒŒì¼**: `backend/app/service_agent/supervisor/team_supervisor.py`

#### í˜„ì¬ ì „ì†¡í•˜ëŠ” Progress ë©”ì‹œì§€

```python
# team_supervisor.py

# âœ… í˜„ì¬ ì „ì†¡ ì¤‘ì¸ ë©”ì‹œì§€
progress_callback("planning_start", {...})           # Planning ì‹œì‘
progress_callback("analysis_start", {...})           # ë¶„ì„ ì‹œì‘
progress_callback("plan_ready", {...})               # ê³„íš ì™„ë£Œ
progress_callback("execution_start", {...})          # ì‹¤í–‰ ì‹œì‘
progress_callback("todo_updated", {...})             # TODO ìƒíƒœ ë³€ê²½
progress_callback("response_generating_start", {...}) # ë‹µë³€ ìƒì„± ì‹œì‘ âœ…
progress_callback("response_generating_progress", {...}) # ë‹µë³€ ìƒì„± ì§„í–‰ âœ…
```

#### ë¬¸ì œê°€ ë˜ëŠ” êµ¬ê°„

**generate_response_node()** ë©”ì„œë“œ:

```python
# Line 1141-1154
async def generate_response_node(self, state: MainSupervisorState) -> Dict[str, Any]:
    state["current_phase"] = "response_generation"

    # âœ… ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ì „ì†¡
    progress_callback = self._progress_callbacks.get(session_id)
    if progress_callback:
        await progress_callback("response_generating_progress", {
            "message": "ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "phase": "response_generation"
        })

    # âŒ LLM í˜¸ì¶œ ì¤‘ 9ì´ˆê°„ ì•„ë¬´ ë©”ì‹œì§€ ì—†ìŒ
    final_response = await self._generate_final_response(
        state,
        team_results,
        planning_state
    )

    # âŒ Summary ìƒì„± ì¤‘ 3ì´ˆê°„ ì•„ë¬´ ë©”ì‹œì§€ ì—†ìŒ
    await self._save_to_long_term_memory(...)

    return {...}
```

**_generate_final_response()** ë©”ì„œë“œ:

```python
# Line 1194-1285
async def _generate_final_response(self, state, team_results, planning_state):
    # âŒ LLM í˜¸ì¶œ ì‹œì‘ ì•Œë¦¼ ì—†ìŒ
    if self.llm_service:
        logger.info("[TeamSupervisor] Using LLM for response generation")

        # ğŸ• 9ì´ˆ ì†Œìš” - ì¤‘ê°„ ì§„í–‰ ìƒíƒœ ì—†ìŒ
        final_response = self.llm_service.generate_final_answer(...)

        logger.info("Final response generated successfully")
    # ...
```

**_save_to_long_term_memory()** ë©”ì„œë“œ:

```python
# Line 1366-1415
async def _save_to_long_term_memory(self, state, final_response_data):
    # âŒ Memory ì €ì¥ ì‹œì‘ ì•Œë¦¼ ì—†ìŒ
    logger.info("[TeamSupervisor] Saving conversation to Long-term Memory")

    # ğŸ• 3ì´ˆ ì†Œìš” (Summary ìƒì„±) - ì¤‘ê°„ ì§„í–‰ ìƒíƒœ ì—†ìŒ
    await memory_service.save_conversation(...)

    logger.info("[TeamSupervisor] Conversation saved to Long-term Memory")
```

### 2. Frontend Progress í‘œì‹œ êµ¬ì¡°

**íŒŒì¼**: `frontend/components/progress-container.tsx`

#### í˜„ì¬ Stage ì§„í–‰ë¥  ê³„ì‚°

```typescript
// Line 53-85
const calculateOverallProgress = (): number => {
  switch (stage) {
    case "dispatch":
      return 10  // ì¶œë™ ì¤‘: 10%

    case "analysis":
      if (plan && plan.execution_steps.length > 0) {
        return 40  // plan_ready ì™„ë£Œ
      }
      return 25  // ë¶„ì„ ì‹œì‘

    case "executing":
      const totalSteps = steps.length
      const completedSteps = steps.filter(s => s.status === "completed").length
      if (totalSteps > 0) {
        const executionProgress = (completedSteps / totalSteps) * 35
        return 40 + executionProgress
      }
      return 40

    case "generating":
      // âŒ ë¬¸ì œ: responsePhaseë§Œìœ¼ë¡œ êµ¬ë¶„, ì„¸ë¶€ ì§„í–‰ ìƒíƒœ ì—†ìŒ
      if (responsePhase === "response_generation") {
        return 90  // ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘
      }
      return 80  // ì •ë³´ ì •ë¦¬ ì¤‘

    default:
      return 0
  }
}
```

**ë¬¸ì œì **:
- `generating` stageì—ì„œ 80% â†’ 90% ë‘ ë‹¨ê³„ë§Œ ì¡´ì¬
- LLM ì‘ë‹µ ìƒì„±(9ì´ˆ), Memory ì €ì¥(3ì´ˆ)ì˜ ì„¸ë¶€ ì§„í–‰ ìƒíƒœ í‘œì‹œ ì—†ìŒ

#### GeneratingContent ì»´í¬ë„ŒíŠ¸

```typescript
// Line 151-233
function GeneratingContent({
  responsePhase
}: {
  responsePhase?: "aggregation" | "response_generation"
}) {
  // âŒ responsePhaseì— ë”°ë¼ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë§Œ ë³€ê²½
  const phaseText = responsePhase === "response_generation"
    ? "ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
    : "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."

  return (
    <div className="space-y-2">
      <div className="flex items-center gap-2 text-sm font-medium">
        <Clock className="w-4 h-4" />
        <span>{phaseText}</span>
      </div>
      {/* ì„¸ë¶€ ì§„í–‰ ìƒíƒœ ì—†ìŒ */}
    </div>
  )
}
```

### 3. WebSocket ë©”ì‹œì§€ ì²˜ë¦¬

**íŒŒì¼**: `frontend/components/chat-interface.tsx`

```typescript
// Line 233-270
case 'response_generating_start':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              stage: "generating" as const,
              responsePhase: message.phase || "aggregation"
            }
          }
        : m
    )
  )
  break

case 'response_generating_progress':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress" && m.progressData?.stage === "generating"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              responsePhase: message.phase || "response_generation"
            }
          }
        : m
    )
  )
  break
```

**ë¬¸ì œì **:
- `responsePhase`ë§Œ ì—…ë°ì´íŠ¸, ì„¸ë¶€ ë‹¨ê³„ ì •ë³´ ì—†ìŒ
- LLM ì‘ì—… ì¤‘, Memory ì €ì¥ ì¤‘ êµ¬ë¶„ ì—†ìŒ

---

## ê°œì„  ë°©ì•ˆ

### ì „ëµ

LLM ì‘ë‹µ ìƒì„± êµ¬ê°„ì„ **5ë‹¨ê³„**ë¡œ ì„¸ë¶„í™”í•˜ì—¬ ì§„í–‰ ìƒíƒœ ì‹¤ì‹œê°„ ì „ì†¡

```
í˜„ì¬ (2ë‹¨ê³„)              ê°œì„  í›„ (5ë‹¨ê³„)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. ì •ë³´ ì •ë¦¬ ì¤‘           1. ì •ë³´ ì •ë¦¬ ì¤‘ (aggregation)
   (80%)                     (80%)

                          2. ë‹µë³€ êµ¬ì¡° ìƒì„± ì¤‘ (structure)
                             (82%)

2. ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘      3. ë‹µë³€ ë‚´ìš© ì‘ì„± ì¤‘ (llm_generating)
   (90%)                     (85%)

                          4. ë‹µë³€ ê²€ì¦ ì¤‘ (validation)
                             (88%)

                          5. ëŒ€í™” ì €ì¥ ì¤‘ (memory_saving)
                             (92%)

                          6. ì™„ë£Œ (100%)
```

### ì¶”ê°€í•  Progress ë©”ì‹œì§€

#### 1. ìƒˆë¡œìš´ ë©”ì‹œì§€ íƒ€ì…

```typescript
// Backend â†’ Frontend WebSocket Messages

{
  "type": "llm_generation_start",
  "message": "ë‹µë³€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
  "phase": "structure",
  "estimated_time": 2  // ì˜ˆìƒ ì†Œìš” ì‹œê°„ (ì´ˆ)
}

{
  "type": "llm_generation_progress",
  "message": "ë‹µë³€ ë‚´ìš©ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
  "phase": "llm_generating",
  "progress": 50  // LLM í† í° ìƒì„± ì§„í–‰ë¥  (ì„ íƒ)
}

{
  "type": "llm_generation_complete",
  "message": "ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
  "phase": "validation",
  "tokens_used": 1431
}

{
  "type": "memory_saving_start",
  "message": "ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
  "phase": "memory_saving"
}

{
  "type": "memory_saving_complete",
  "message": "ëŒ€í™” ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
  "phase": "complete"
}
```

#### 2. ì„¸ë¶€ Phase ì •ì˜

| Phase | ì„¤ëª… | ì˜ˆìƒ ì†Œìš” ì‹œê°„ | ì§„í–‰ë¥  |
|-------|------|----------------|--------|
| `aggregation` | ìˆ˜ì§‘ ì •ë³´ ì •ë¦¬ | 1ì´ˆ | 80% |
| `structure` | ë‹µë³€ êµ¬ì¡° ìƒì„± | 1ì´ˆ | 82% |
| `llm_generating` | LLM ë‹µë³€ ì‘ì„± | 6-8ì´ˆ | 85% |
| `validation` | ë‹µë³€ ê²€ì¦ ë° í¬ë§·íŒ… | 1ì´ˆ | 88% |
| `memory_saving` | ëŒ€í™” ì €ì¥ ë° ìš”ì•½ | 3ì´ˆ | 92% |
| `complete` | ì™„ë£Œ | - | 100% |

---

## êµ¬í˜„ ê³„íš

### Phase 1: Backend Progress ë©”ì‹œì§€ ì¶”ê°€

#### Task 1.1: generate_response_node() ê°œì„ 

**íŒŒì¼**: `backend/app/service_agent/supervisor/team_supervisor.py`

**í˜„ì¬ ì½”ë“œ (Line 1141-1285)**:

```python
async def generate_response_node(self, state: MainSupervisorState) -> Dict[str, Any]:
    state["current_phase"] = "response_generation"

    # ê¸°ì¡´: í•œ ë²ˆë§Œ ì „ì†¡
    progress_callback = self._progress_callbacks.get(session_id)
    if progress_callback:
        await progress_callback("response_generating_progress", {
            "message": "ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "phase": "response_generation"
        })

    # LLM í˜¸ì¶œ
    final_response = await self._generate_final_response(...)

    # Memory ì €ì¥
    await self._save_to_long_term_memory(...)

    return {...}
```

**ê°œì„  í›„**:

```python
async def generate_response_node(self, state: MainSupervisorState) -> Dict[str, Any]:
    state["current_phase"] = "response_generation"
    session_id = state.get("session_id")
    progress_callback = self._progress_callbacks.get(session_id)

    # âœ… Step 1: ì •ë³´ ì •ë¦¬ ì¤‘ (aggregation)
    if progress_callback:
        await progress_callback("response_generating_progress", {
            "message": "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "phase": "aggregation",
            "progress_percent": 80
        })

    # Team results ì§‘ê³„
    team_results = self._collect_team_results(state)

    # âœ… Step 2: ë‹µë³€ êµ¬ì¡° ìƒì„± ì¤‘ (structure)
    if progress_callback:
        await progress_callback("llm_generation_start", {
            "message": "ë‹µë³€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "phase": "structure",
            "progress_percent": 82,
            "estimated_time": 2
        })

    # LLM í˜¸ì¶œ ì „ ì¤€ë¹„
    prompt_data = self._prepare_llm_prompt(state, team_results, planning_state)

    # âœ… Step 3: LLM ë‹µë³€ ì‘ì„± ì¤‘ (llm_generating)
    if progress_callback:
        await progress_callback("llm_generation_progress", {
            "message": "ë‹µë³€ ë‚´ìš©ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "phase": "llm_generating",
            "progress_percent": 85,
            "estimated_time": 8
        })

    # LLM í˜¸ì¶œ
    final_response = await self._generate_final_response(
        state, team_results, planning_state
    )

    # âœ… Step 4: ë‹µë³€ ê²€ì¦ ì¤‘ (validation)
    if progress_callback:
        await progress_callback("llm_generation_complete", {
            "message": "ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "phase": "validation",
            "progress_percent": 88,
            "tokens_used": final_response.get("tokens_used", 0)
        })

    # ë‹µë³€ ê²€ì¦ ë° í¬ë§·íŒ…
    validated_response = self._validate_response(final_response)

    # âœ… Step 5: ëŒ€í™” ì €ì¥ ì¤‘ (memory_saving)
    if progress_callback:
        await progress_callback("memory_saving_start", {
            "message": "ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "phase": "memory_saving",
            "progress_percent": 92
        })

    # Memory ì €ì¥
    await self._save_to_long_term_memory(
        state,
        validated_response.get("answer", ""),
        final_response
    )

    # âœ… Step 6: ì™„ë£Œ
    if progress_callback:
        await progress_callback("memory_saving_complete", {
            "message": "ëŒ€í™” ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "phase": "complete",
            "progress_percent": 95
        })

    return {
        "final_response": validated_response,
        "workflow_status": "completed"
    }
```

**ì¶”ê°€ í•„ìš” Helper ë©”ì„œë“œ**:

```python
def _collect_team_results(self, state: MainSupervisorState) -> Dict[str, Any]:
    """íŒ€ ì‹¤í–‰ ê²°ê³¼ ìˆ˜ì§‘"""
    # ê¸°ì¡´ ë¡œì§ ë¶„ë¦¬
    pass

def _prepare_llm_prompt(
    self,
    state: MainSupervisorState,
    team_results: Dict,
    planning_state: Dict
) -> str:
    """LLM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
    # ê¸°ì¡´ ë¡œì§ ë¶„ë¦¬
    pass

def _validate_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
    """ì‘ë‹µ ê²€ì¦ ë° í¬ë§·íŒ…"""
    # ì‘ë‹µ êµ¬ì¡° ê²€ì¦, í•„ìˆ˜ í•„ë“œ ì²´í¬ ë“±
    return response
```

#### Task 1.2: _generate_final_response() ê°œì„ 

**í˜„ì¬ ì½”ë“œ**:

```python
async def _generate_final_response(self, state, team_results, planning_state):
    if self.llm_service:
        logger.info("[TeamSupervisor] Using LLM for response generation")

        # ğŸ• 9ì´ˆ ì†Œìš” - ì§„í–‰ ìƒíƒœ ì—†ìŒ
        final_response = self.llm_service.generate_final_answer(...)

        logger.info("Final response generated successfully")
    # ...
```

**ê°œì„  í›„**:

```python
async def _generate_final_response(
    self,
    state: MainSupervisorState,
    team_results: Dict,
    planning_state: Dict
) -> Dict[str, Any]:
    """
    LLMì„ ì‚¬ìš©í•œ ìµœì¢… ì‘ë‹µ ìƒì„± (ì§„í–‰ ìƒíƒœ ì „ì†¡ í¬í•¨)
    """
    session_id = state.get("session_id")
    progress_callback = self._progress_callbacks.get(session_id)

    if self.llm_service:
        logger.info("[TeamSupervisor] Using LLM for response generation")

        # âœ… LLM í˜¸ì¶œ ì§ì „ ì•Œë¦¼ (ì„ íƒì  - ì´ë¯¸ llm_generation_progressì—ì„œ ì „ì†¡)
        # if progress_callback:
        #     await progress_callback("llm_calling", {
        #         "message": "LLMì— ì§ˆì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
        #         "model": self.llm_service.model_name
        #     })

        # LLM í˜¸ì¶œ (streaming ì§€ì› ì‹œ ì¤‘ê°„ ì§„í–‰ ì „ì†¡ ê°€ëŠ¥)
        try:
            final_response = self.llm_service.generate_final_answer(
                query=state.get("query", ""),
                team_results=team_results,
                planning_state=planning_state,
                chat_history=state.get("chat_history", [])
            )

            logger.info(f"Final response generated successfully for query: {state.get('query', '')[:50]}...")

            return final_response

        except Exception as e:
            logger.error(f"LLM response generation failed: {e}", exc_info=True)

            # ì—ëŸ¬ ì‹œ progress callback ì „ì†¡
            if progress_callback:
                await progress_callback("error", {
                    "message": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "error": str(e)
                })

            # Fallback ì‘ë‹µ
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "type": "error",
                "sections": []
            }
    # ...
```

**í–¥í›„ ê°œì„  (LLM Streaming ì§€ì› ì‹œ)**:

```python
# LLMServiceì— streaming callback ì¶”ê°€
final_response = await self.llm_service.generate_final_answer_streaming(
    query=state.get("query", ""),
    team_results=team_results,
    planning_state=planning_state,
    chat_history=state.get("chat_history", []),
    # âœ… Streaming callback
    on_token=lambda token, total_tokens: asyncio.create_task(
        progress_callback("llm_token_generated", {
            "tokens_generated": total_tokens,
            "progress_percent": min(85 + (total_tokens / 1500) * 3, 87)
        }) if progress_callback else None
    )
)
```

#### Task 1.3: _save_to_long_term_memory() ê°œì„ 

**í˜„ì¬ ì½”ë“œ**:

```python
async def _save_to_long_term_memory(self, state, final_response_data):
    logger.info("[TeamSupervisor] Saving conversation to Long-term Memory")

    # ğŸ• 3ì´ˆ ì†Œìš” - ì§„í–‰ ìƒíƒœ ì—†ìŒ
    await memory_service.save_conversation(...)

    logger.info("[TeamSupervisor] Conversation saved to Long-term Memory")
```

**ê°œì„  í›„**:

```python
async def _save_to_long_term_memory(
    self,
    state: MainSupervisorState,
    final_answer: str,
    final_response_data: Dict[str, Any]
) -> None:
    """
    Long-term Memoryì— ëŒ€í™” ì €ì¥ (ì§„í–‰ ìƒíƒœ ì „ì†¡ í¬í•¨)
    """
    user_id = state.get("user_id")
    chat_session_id = state.get("chat_session_id")
    session_id = state.get("session_id")

    if not user_id or not chat_session_id:
        logger.warning("[TeamSupervisor] Skipping long-term memory: user_id or chat_session_id missing")
        return

    logger.info(f"[TeamSupervisor] Saving conversation to Long-term Memory for user {user_id}")

    progress_callback = self._progress_callbacks.get(session_id)

    # âœ… Memory ì €ì¥ ì‹œì‘ ì•Œë¦¼ (ì´ë¯¸ generate_response_nodeì—ì„œ ì „ì†¡ë¨)
    # if progress_callback:
    #     await progress_callback("memory_saving_start", {...})

    try:
        async for db in get_async_db():
            try:
                from app.service_agent.foundation.simple_memory_service import LongTermMemoryService

                memory_service = LongTermMemoryService()

                # âœ… Background taskë¡œ summary ìƒì„± (ë¹„ë™ê¸°)
                summary_task = asyncio.create_task(
                    memory_service.save_conversation(
                        db=db,
                        user_id=user_id,
                        session_id=chat_session_id,
                        query=state.get("query", ""),
                        response=final_answer
                    )
                )

                logger.info(f"[TeamSupervisor] Background summary started for session: {chat_session_id}")

                # âœ… Summary ì§„í–‰ ìƒíƒœ ì „ì†¡ (ì„ íƒì )
                if progress_callback:
                    await progress_callback("summary_generating", {
                        "message": "ëŒ€í™” ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                        "phase": "summary",
                        "progress_percent": 93
                    })

                # Background task ì™„ë£Œ ëŒ€ê¸° (optional, í˜„ì¬ëŠ” non-blocking)
                # await summary_task

                logger.info("[TeamSupervisor] Conversation saved to Long-term Memory")

            except Exception as e:
                logger.error(f"Failed to save to long-term memory: {e}", exc_info=True)
                await db.rollback()

                # ì—ëŸ¬ ë°œìƒí•´ë„ ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ (ë©”ëª¨ë¦¬ëŠ” optional)
                if progress_callback:
                    await progress_callback("warning", {
                        "message": "ëŒ€í™” ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìœ¼ë‚˜ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.",
                        "error": str(e)
                    })

            finally:
                break

    except Exception as e:
        logger.error(f"Failed to access database: {e}", exc_info=True)
```

---

### Phase 2: Frontend Progress UI ê°œì„ 

#### Task 2.1: WebSocket ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ ì¶”ê°€

**íŒŒì¼**: `frontend/components/chat-interface.tsx`

**ì¶”ê°€í•  ì¼€ì´ìŠ¤**:

```typescript
// Line 233 ì´í›„ ì¶”ê°€

case 'llm_generation_start':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              stage: "generating" as const,
              generatingPhase: message.phase || "structure",
              progressPercent: message.progress_percent || 82,
              estimatedTime: message.estimated_time
            }
          }
        : m
    )
  )
  setProcessState({
    step: "generating_response",
    agentType: null,
    message: message.message || "ë‹µë³€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
  })
  break

case 'llm_generation_progress':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress" && m.progressData?.stage === "generating"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              generatingPhase: message.phase || "llm_generating",
              progressPercent: message.progress_percent || 85,
              tokensGenerated: message.tokens_generated
            }
          }
        : m
    )
  )
  setProcessState({
    step: "generating_response",
    agentType: null,
    message: message.message || "ë‹µë³€ ë‚´ìš©ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
  })
  break

case 'llm_generation_complete':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress" && m.progressData?.stage === "generating"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              generatingPhase: message.phase || "validation",
              progressPercent: message.progress_percent || 88,
              tokensUsed: message.tokens_used
            }
          }
        : m
    )
  )
  setProcessState({
    step: "generating_response",
    agentType: null,
    message: message.message || "ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
  })
  break

case 'memory_saving_start':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress" && m.progressData?.stage === "generating"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              generatingPhase: message.phase || "memory_saving",
              progressPercent: message.progress_percent || 92
            }
          }
        : m
    )
  )
  setProcessState({
    step: "saving_memory",
    agentType: null,
    message: message.message || "ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
  })
  break

case 'memory_saving_complete':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress" && m.progressData?.stage === "generating"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              generatingPhase: "complete",
              progressPercent: message.progress_percent || 95
            }
          }
        : m
    )
  )
  setProcessState({
    step: "completed",
    agentType: null,
    message: "ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
  })
  break

case 'summary_generating':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress" && m.progressData?.stage === "generating"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              generatingPhase: "summary",
              progressPercent: message.progress_percent || 93
            }
          }
        : m
    )
  )
  break
```

#### Task 2.2: ProgressData íƒ€ì… í™•ì¥

**íŒŒì¼**: `frontend/types/execution.ts` (ë˜ëŠ” ê´€ë ¨ íƒ€ì… ì •ì˜ íŒŒì¼)

```typescript
export type GeneratingPhase =
  | "aggregation"      // ì •ë³´ ì •ë¦¬
  | "structure"        // êµ¬ì¡° ìƒì„±
  | "llm_generating"   // LLM ë‹µë³€ ì‘ì„±
  | "validation"       // ë‹µë³€ ê²€ì¦
  | "memory_saving"    // ëŒ€í™” ì €ì¥
  | "summary"          // ìš”ì•½ ìƒì„±
  | "complete"         // ì™„ë£Œ

export interface ProgressData {
  stage: ProgressStage
  plan?: ExecutionPlan
  steps?: ExecutionStep[]
  responsePhase?: "aggregation" | "response_generation"  // ê¸°ì¡´

  // âœ… ìƒˆë¡œ ì¶”ê°€
  generatingPhase?: GeneratingPhase
  progressPercent?: number
  estimatedTime?: number
  tokensGenerated?: number
  tokensUsed?: number
}
```

#### Task 2.3: ProgressContainer ì§„í–‰ë¥  ê³„ì‚° ê°œì„ 

**íŒŒì¼**: `frontend/components/progress-container.tsx`

**í˜„ì¬ ì½”ë“œ (Line 75-84)**:

```typescript
case "generating":
  // ë‹µë³€ ì‘ì„± ì¤‘: 75-95%
  if (responsePhase === "response_generation") {
    return 90  // ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘
  }
  return 80  // ì •ë³´ ì •ë¦¬ ì¤‘
```

**ê°œì„  í›„**:

```typescript
case "generating":
  // ë‹µë³€ ì‘ì„± ì¤‘: 80-95%
  // âœ… Backendì—ì„œ ì „ì†¡í•œ progressPercent ì‚¬ìš©
  if (progressData?.progressPercent) {
    return progressData.progressPercent
  }

  // Fallback: generatingPhase ê¸°ë°˜ ê³„ì‚°
  const generatingPhase = progressData?.generatingPhase || "aggregation"

  switch (generatingPhase) {
    case "aggregation":
      return 80  // ì •ë³´ ì •ë¦¬
    case "structure":
      return 82  // êµ¬ì¡° ìƒì„±
    case "llm_generating":
      return 85  // LLM ë‹µë³€ ì‘ì„±
    case "validation":
      return 88  // ë‹µë³€ ê²€ì¦
    case "memory_saving":
      return 92  // ëŒ€í™” ì €ì¥
    case "summary":
      return 93  // ìš”ì•½ ìƒì„±
    case "complete":
      return 95  // ì™„ë£Œ
    default:
      return 80
  }
```

#### Task 2.4: GeneratingContent ì»´í¬ë„ŒíŠ¸ ê°œì„ 

**íŒŒì¼**: `frontend/components/progress-container.tsx`

**í˜„ì¬ ì½”ë“œ (Line 151-233)**:

```typescript
function GeneratingContent({
  responsePhase
}: {
  responsePhase?: "aggregation" | "response_generation"
}) {
  const phaseText = responsePhase === "response_generation"
    ? "ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
    : "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."

  return (
    <div className="space-y-2">
      <div className="flex items-center gap-2 text-sm font-medium">
        <Clock className="w-4 h-4" />
        <span>{phaseText}</span>
      </div>
    </div>
  )
}
```

**ê°œì„  í›„**:

```typescript
function GeneratingContent({
  responsePhase,
  generatingPhase,
  progressPercent,
  estimatedTime,
  tokensGenerated,
  tokensUsed
}: {
  responsePhase?: "aggregation" | "response_generation"
  generatingPhase?: GeneratingPhase
  progressPercent?: number
  estimatedTime?: number
  tokensGenerated?: number
  tokensUsed?: number
}) {
  // âœ… generatingPhase ê¸°ë°˜ ë©”ì‹œì§€ ìƒì„±
  const getPhaseMessage = () => {
    if (!generatingPhase) {
      // Fallback: ê¸°ì¡´ responsePhase ì‚¬ìš©
      return responsePhase === "response_generation"
        ? "ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        : "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
    }

    switch (generatingPhase) {
      case "aggregation":
        return "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
      case "structure":
        return "ë‹µë³€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
      case "llm_generating":
        return "ë‹µë³€ ë‚´ìš©ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
      case "validation":
        return "ë‹µë³€ì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
      case "memory_saving":
        return "ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
      case "summary":
        return "ëŒ€í™” ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
      case "complete":
        return "ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
      default:
        return "ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."
    }
  }

  const phaseMessage = getPhaseMessage()

  // âœ… Phaseë³„ ì•„ì´ì½˜ ì„ íƒ
  const getPhaseIcon = () => {
    switch (generatingPhase) {
      case "aggregation":
        return <Layers className="w-4 h-4" />
      case "structure":
        return <FileText className="w-4 h-4" />
      case "llm_generating":
        return <Sparkles className="w-4 h-4 animate-pulse" />
      case "validation":
        return <CheckCircle className="w-4 h-4" />
      case "memory_saving":
      case "summary":
        return <Database className="w-4 h-4" />
      case "complete":
        return <CheckCircle2 className="w-4 h-4 text-green-500" />
      default:
        return <Clock className="w-4 h-4" />
    }
  }

  return (
    <div className="space-y-3">
      {/* í˜„ì¬ Phase ë©”ì‹œì§€ */}
      <div className="flex items-center gap-2 text-sm font-medium">
        {getPhaseIcon()}
        <span>{phaseMessage}</span>
      </div>

      {/* ì„¸ë¶€ ì§„í–‰ ìƒíƒœ */}
      <div className="space-y-2">
        {/* ì§„í–‰ë¥  í‘œì‹œ (progressPercent ìˆì„ ê²½ìš°) */}
        {progressPercent !== undefined && (
          <div className="flex items-center justify-between text-xs text-muted-foreground">
            <span>ì§„í–‰ë¥ </span>
            <span className="font-semibold">{progressPercent}%</span>
          </div>
        )}

        {/* ì˜ˆìƒ ì‹œê°„ í‘œì‹œ (estimatedTime ìˆì„ ê²½ìš°) */}
        {estimatedTime !== undefined && generatingPhase !== "complete" && (
          <div className="flex items-center justify-between text-xs text-muted-foreground">
            <span>ì˜ˆìƒ ì†Œìš” ì‹œê°„</span>
            <span>{estimatedTime}ì´ˆ</span>
          </div>
        )}

        {/* í† í° ìƒì„± ì§„í–‰ (tokensGenerated ìˆì„ ê²½ìš°) */}
        {tokensGenerated !== undefined && (
          <div className="flex items-center justify-between text-xs text-muted-foreground">
            <span>ìƒì„±ëœ í† í°</span>
            <span className="font-mono">{tokensGenerated.toLocaleString()}</span>
          </div>
        )}

        {/* ì´ í† í° ì‚¬ìš©ëŸ‰ (tokensUsed ìˆì„ ê²½ìš°) */}
        {tokensUsed !== undefined && (
          <div className="flex items-center justify-between text-xs text-muted-foreground">
            <span>ì´ í† í°</span>
            <span className="font-mono">{tokensUsed.toLocaleString()}</span>
          </div>
        )}

        {/* Phaseë³„ ì„¸ë¶€ ë‹¨ê³„ í‘œì‹œ */}
        {generatingPhase && (
          <div className="mt-3 space-y-1">
            <PhaseSteps currentPhase={generatingPhase} />
          </div>
        )}
      </div>
    </div>
  )
}

// âœ… Phaseë³„ ì„¸ë¶€ ë‹¨ê³„ í‘œì‹œ ì»´í¬ë„ŒíŠ¸
function PhaseSteps({ currentPhase }: { currentPhase: GeneratingPhase }) {
  const phases = [
    { id: "aggregation", label: "ì •ë³´ ì •ë¦¬", icon: "ğŸ“Š" },
    { id: "structure", label: "êµ¬ì¡° ìƒì„±", icon: "ğŸ—ï¸" },
    { id: "llm_generating", label: "ë‚´ìš© ì‘ì„±", icon: "âœï¸" },
    { id: "validation", label: "ê²€ì¦", icon: "âœ…" },
    { id: "memory_saving", label: "ì €ì¥", icon: "ğŸ’¾" },
  ] as const

  const currentIndex = phases.findIndex(p => p.id === currentPhase)

  return (
    <div className="flex items-center gap-1">
      {phases.map((phase, idx) => (
        <div
          key={phase.id}
          className={`
            flex-1 h-1.5 rounded-full transition-all duration-300
            ${
              idx < currentIndex
                ? "bg-primary"  // ì™„ë£Œëœ ë‹¨ê³„
                : idx === currentIndex
                ? "bg-primary/70 animate-pulse"  // í˜„ì¬ ë‹¨ê³„
                : "bg-muted"  // ëŒ€ê¸° ì¤‘ ë‹¨ê³„
            }
          `}
          title={`${phase.icon} ${phase.label}`}
        />
      ))}
    </div>
  )
}
```

**í•„ìš”í•œ ì•„ì´ì½˜ import**:

```typescript
import {
  Clock,
  Layers,
  FileText,
  Sparkles,
  CheckCircle,
  CheckCircle2,
  Database
} from "lucide-react"
```

---

### Phase 3: ì¶”ê°€ ê°œì„  (ì„ íƒì )

#### Task 3.1: LLM Streaming ì§€ì› (í–¥í›„)

**ëª©í‘œ**: LLM í† í° ìƒì„±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ

**êµ¬í˜„ ë°©ë²•**:
1. LLMServiceì— streaming API ì¶”ê°€
2. í† í° ìƒì„±ë§ˆë‹¤ WebSocket ë©”ì‹œì§€ ì „ì†¡
3. Frontendì—ì„œ ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸

**ì˜ˆì‹œ**:

```python
# Backend
async def generate_final_answer_streaming(
    self,
    query: str,
    on_token: Callable[[str, int], Awaitable[None]] = None
):
    """LLM ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„±"""
    total_tokens = 0

    async for token in self.llm_client.stream(prompt):
        total_tokens += 1

        # 10 í† í°ë§ˆë‹¤ ì§„í–‰ ìƒíƒœ ì „ì†¡
        if on_token and total_tokens % 10 == 0:
            await on_token(token, total_tokens)

        # í† í° ëˆ„ì 
        ...
```

```typescript
// Frontend
case 'llm_token_generated':
  setMessages((prev) =>
    prev.map(m =>
      m.type === "progress" && m.progressData?.stage === "generating"
        ? {
            ...m,
            progressData: {
              ...m.progressData,
              tokensGenerated: message.tokens_generated,
              progressPercent: message.progress_percent
            }
          }
        : m
    )
  )
  break
```

#### Task 3.2: ì˜ˆìƒ ì‹œê°„ ë™ì  ê³„ì‚°

**ëª©í‘œ**: ê³¼ê±° ì‹¤í–‰ ì´ë ¥ ê¸°ë°˜ ì˜ˆìƒ ì‹œê°„ ê³„ì‚°

**êµ¬í˜„ ë°©ë²•**:
1. PostgreSQLì— ì‹¤í–‰ ì‹œê°„ í†µê³„ ì €ì¥
2. í‰ê·  ì†Œìš” ì‹œê°„ ê³„ì‚°
3. Progress ë©”ì‹œì§€ì— ì˜ˆìƒ ì‹œê°„ í¬í•¨

**ì˜ˆì‹œ**:

```python
# í‰ê·  LLM ì‘ë‹µ ìƒì„± ì‹œê°„ ì¡°íšŒ
avg_llm_time = await self._get_average_llm_time(user_id)

if progress_callback:
    await progress_callback("llm_generation_progress", {
        "message": "ë‹µë³€ ë‚´ìš©ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
        "phase": "llm_generating",
        "estimated_time": avg_llm_time  # ë™ì  ê³„ì‚°
    })
```

#### Task 3.3: ì—ëŸ¬ ë°œìƒ ì‹œ Progress í‘œì‹œ

**ëª©í‘œ**: LLM ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ìƒíƒœ ì „ë‹¬

**êµ¬í˜„ ë°©ë²•**:

```python
try:
    final_response = self.llm_service.generate_final_answer(...)
except Exception as e:
    logger.error(f"LLM generation failed: {e}")

    if progress_callback:
        await progress_callback("error", {
            "message": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "error": str(e),
            "phase": "llm_generating",
            "recovery_suggestion": "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        })

    # Fallback ì‘ë‹µ
    return self._create_fallback_response()
```

---

## ì˜ˆìƒ íš¨ê³¼

### ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

**Before**:
```
ë‹µë³€ ì‘ì„± ì¤‘... (12ì´ˆê°„ ë³€í™” ì—†ìŒ) ğŸ˜°
```

**After**:
```
ì •ë³´ ì •ë¦¬ ì¤‘...              [80%] â±ï¸ 1ì´ˆ
ë‹µë³€ êµ¬ì¡° ìƒì„± ì¤‘...          [82%] â±ï¸ 2ì´ˆ
ë‹µë³€ ë‚´ìš© ì‘ì„± ì¤‘...          [85%] âœï¸ 6ì´ˆ
ë‹µë³€ ê²€ì¦ ì¤‘...              [88%] âœ… 1ì´ˆ
ëŒ€í™” ì €ì¥ ì¤‘...              [92%] ğŸ’¾ 3ì´ˆ
ì™„ë£Œ! ğŸ‰
```

### ì •ëŸ‰ì  ê°œì„ 

| ì§€í‘œ | Before | After | ê°œì„  |
|------|--------|-------|------|
| ì²´ê° ëŒ€ê¸° ì‹œê°„ | 12ì´ˆ | 5ì´ˆ | -58% |
| ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ | 1íšŒ | 5íšŒ | +400% |
| ì‚¬ìš©ì ì´íƒˆë¥  (ì˜ˆìƒ) | 15% | 5% | -67% |
| UX ë§Œì¡±ë„ (ì˜ˆìƒ) | 3.0/5 | 4.5/5 | +50% |

### ê¸°ìˆ ì  ì´ì 

1. **íˆ¬ëª…ì„± í–¥ìƒ**: ê° ì²˜ë¦¬ ë‹¨ê³„ê°€ ëª…í™•íˆ í‘œì‹œë¨
2. **ë””ë²„ê¹… ìš©ì´**: ì–´ëŠ ë‹¨ê³„ì—ì„œ ì§€ì—°ë˜ëŠ”ì§€ íŒŒì•… ê°€ëŠ¥
3. **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ ë‹¨ê³„ ì¶”ê°€ ìš©ì´
4. **ì—ëŸ¬ í•¸ë“¤ë§**: ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬ ëª…í™•í™”

---

## êµ¬í˜„ ì¼ì •

| Phase | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ìš°ì„ ìˆœìœ„ |
|-------|------|----------|---------|
| **Phase 1** | Backend Progress ë©”ì‹œì§€ ì¶”ê°€ | 4ì‹œê°„ | P1 |
| Task 1.1 | generate_response_node() ê°œì„  | 2ì‹œê°„ | P1 |
| Task 1.2 | _generate_final_response() ê°œì„  | 1ì‹œê°„ | P1 |
| Task 1.3 | _save_to_long_term_memory() ê°œì„  | 1ì‹œê°„ | P1 |
| **Phase 2** | Frontend Progress UI ê°œì„  | 6ì‹œê°„ | P1 |
| Task 2.1 | WebSocket ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ ì¶”ê°€ | 2ì‹œê°„ | P1 |
| Task 2.2 | ProgressData íƒ€ì… í™•ì¥ | 1ì‹œê°„ | P1 |
| Task 2.3 | ProgressContainer ì§„í–‰ë¥  ê³„ì‚° ê°œì„  | 1ì‹œê°„ | P1 |
| Task 2.4 | GeneratingContent ì»´í¬ë„ŒíŠ¸ ê°œì„  | 2ì‹œê°„ | P1 |
| **Phase 3** | ì¶”ê°€ ê°œì„  (ì„ íƒì ) | 8ì‹œê°„ | P2 |
| Task 3.1 | LLM Streaming ì§€ì› | 4ì‹œê°„ | P2 |
| Task 3.2 | ì˜ˆìƒ ì‹œê°„ ë™ì  ê³„ì‚° | 2ì‹œê°„ | P2 |
| Task 3.3 | ì—ëŸ¬ ë°œìƒ ì‹œ Progress í‘œì‹œ | 2ì‹œê°„ | P2 |
| **Total** | | **18ì‹œê°„** (P1: 10ì‹œê°„) | |

---

## í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# tests/test_progress_callbacks.py

@pytest.mark.asyncio
async def test_generate_response_progress_callbacks():
    """generate_response_nodeì—ì„œ ëª¨ë“  progress callbackì´ í˜¸ì¶œë˜ëŠ”ì§€ í™•ì¸"""
    supervisor = TeamBasedSupervisor()

    # Mock progress callback
    progress_events = []

    async def mock_callback(event_type: str, event_data: dict):
        progress_events.append((event_type, event_data))

    supervisor._progress_callbacks["test_session"] = mock_callback

    state = {
        "session_id": "test_session",
        "query": "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬",
        # ... other state
    }

    # Execute node
    result = await supervisor.generate_response_node(state)

    # Verify all progress events sent
    expected_events = [
        "response_generating_progress",  # aggregation
        "llm_generation_start",          # structure
        "llm_generation_progress",       # llm_generating
        "llm_generation_complete",       # validation
        "memory_saving_start",           # memory_saving
        "memory_saving_complete"         # complete
    ]

    actual_event_types = [e[0] for e in progress_events]

    for expected_event in expected_events:
        assert expected_event in actual_event_types, \
            f"Expected event '{expected_event}' not found in {actual_event_types}"
```

### í†µí•© í…ŒìŠ¤íŠ¸

```typescript
// frontend/tests/progress-display.test.tsx

describe('Progress Display Integration', () => {
  it('should update progress through all generating phases', async () => {
    const { getByText } = render(<ChatInterface />)

    // Simulate WebSocket messages
    const ws = mockWebSocket()

    // Phase 1: aggregation
    ws.send({ type: 'response_generating_progress', phase: 'aggregation', progress_percent: 80 })
    await waitFor(() => {
      expect(getByText('ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...')).toBeInTheDocument()
      expect(getByText('80%')).toBeInTheDocument()
    })

    // Phase 2: structure
    ws.send({ type: 'llm_generation_start', phase: 'structure', progress_percent: 82 })
    await waitFor(() => {
      expect(getByText('ë‹µë³€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...')).toBeInTheDocument()
      expect(getByText('82%')).toBeInTheDocument()
    })

    // Phase 3: llm_generating
    ws.send({ type: 'llm_generation_progress', phase: 'llm_generating', progress_percent: 85 })
    await waitFor(() => {
      expect(getByText('ë‹µë³€ ë‚´ìš©ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...')).toBeInTheDocument()
      expect(getByText('85%')).toBeInTheDocument()
    })

    // ... and so on
  })
})
```

---

## ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

| ë¦¬ìŠ¤í¬ | í™•ë¥  | ì˜í–¥ë„ | ëŒ€ì‘ ë°©ì•ˆ |
|--------|------|--------|-----------|
| WebSocket ë©”ì‹œì§€ ì§€ì—°ìœ¼ë¡œ ìˆœì„œ ë’¤ë°”ë€œ | ë‚®ìŒ | ì¤‘ê°„ | ë©”ì‹œì§€ì— timestamp ì¶”ê°€, ìˆœì„œ ê²€ì¦ |
| LLM í˜¸ì¶œ ì‹œê°„ í¸ì°¨ë¡œ ì§„í–‰ë¥  ë¶€ì •í™• | ì¤‘ê°„ | ë‚®ìŒ | ì˜ˆìƒ ì‹œê°„ ë™ì  ì¡°ì • (Phase 3) |
| Progress ë©”ì‹œì§€ ê³¼ë‹¤ë¡œ ì„±ëŠ¥ ì €í•˜ | ë‚®ìŒ | ë‚®ìŒ | ë©”ì‹œì§€ throttling (100ms ê°„ê²©) |
| Frontend rendering ë¶€í•˜ | ë‚®ìŒ | ë‚®ìŒ | React.memoë¡œ ìµœì í™” |

---

## ê²°ë¡ 

### í•µì‹¬ ê°œì„  ì‚¬í•­

1. âœ… **LLM ì‘ë‹µ ìƒì„± êµ¬ê°„ 5ë‹¨ê³„ë¡œ ì„¸ë¶„í™”**
   - aggregation â†’ structure â†’ llm_generating â†’ validation â†’ memory_saving

2. âœ… **ì‹¤ì‹œê°„ ì§„í–‰ ìƒíƒœ WebSocket ì „ì†¡**
   - ê¸°ì¡´ 1íšŒ â†’ ê°œì„  í›„ 5íšŒ (5ë°° ì¦ê°€)

3. âœ… **ì§„í–‰ë¥  80% â†’ 95% ì„¸ë°€í•˜ê²Œ í‘œì‹œ**
   - 2% ë‹¨ìœ„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸

4. âœ… **ì˜ˆìƒ ì†Œìš” ì‹œê°„ í‘œì‹œ**
   - ì‚¬ìš©ì ëŒ€ê¸° ë¶ˆì•ˆê° í•´ì†Œ

### ê¸°ëŒ€ íš¨ê³¼

- **ì²´ê° ëŒ€ê¸° ì‹œê°„ 58% ê°ì†Œ** (12ì´ˆ â†’ 5ì´ˆ ëŠë‚Œ)
- **ì‚¬ìš©ì ì´íƒˆë¥  67% ê°ì†Œ** (ì˜ˆìƒ)
- **UX ë§Œì¡±ë„ 50% í–¥ìƒ** (ì˜ˆìƒ)

### ë‹¤ìŒ ë‹¨ê³„

1. **Phase 1 êµ¬í˜„** (Backend) - 4ì‹œê°„
2. **Phase 2 êµ¬í˜„** (Frontend) - 6ì‹œê°„
3. **í…ŒìŠ¤íŠ¸ ë° ë°°í¬** - 2ì‹œê°„
4. **Phase 3 êµ¬í˜„** (ì„ íƒì ) - 8ì‹œê°„

---

**ì‘ì„±ì**: Holmes AI Team
**ìŠ¹ì¸**: Pending
**ê´€ë ¨ ë¬¸ì„œ**:
- DOCUMENT_EXECUTOR_REFACTORING_PLAN_251026.md
- VALIDATION_COMPLIANCE_TOOLS_PLAN_251026.md
