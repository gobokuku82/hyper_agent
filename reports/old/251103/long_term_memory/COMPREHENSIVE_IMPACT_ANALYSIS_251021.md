# 3-Tier Hybrid Memory ì¢…í•© ì˜í–¥ ë¶„ì„ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025-10-21
**ì‘ì„±ì**: Claude Code Analysis
**ë¶„ì„ ë²”ìœ„**: ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ëŒ€ì¡° ë° ì„¸ë¶€ ë¶„ì„
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 3ì‹œê°„ 45ë¶„

---

## ğŸ“‹ ëª©ì°¨

1. [ë¶„ì„ ìš”ì•½](#ë¶„ì„-ìš”ì•½)
2. [í˜„ì¬ ì½”ë“œ ìƒíƒœ ê²€ì¦](#í˜„ì¬-ì½”ë“œ-ìƒíƒœ-ê²€ì¦)
3. [ì˜í–¥ ë°›ëŠ” íŒŒì¼ ì „ì²´ ëª©ë¡](#ì˜í–¥-ë°›ëŠ”-íŒŒì¼-ì „ì²´-ëª©ë¡)
4. [ì½”ë“œ íë¦„ ë¶„ì„](#ì½”ë“œ-íë¦„-ë¶„ì„)
5. [êµ¬í˜„ ê³„íš ê²€ì¦](#êµ¬í˜„-ê³„íš-ê²€ì¦)
6. [ì ì¬ì  ì´ìŠˆ ë° í•´ê²°ë°©ì•ˆ](#ì ì¬ì -ì´ìŠˆ-ë°-í•´ê²°ë°©ì•ˆ)
7. [ì¢…í•© ì²´í¬ë¦¬ìŠ¤íŠ¸](#ì¢…í•©-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## ë¶„ì„ ìš”ì•½

### ğŸ¯ í•µì‹¬ ë°œê²¬ì‚¬í•­

#### âœ… ê³„íšì„œ ì •í™•ë„: 95%
- **ì •í™•í•œ ë¶€ë¶„**: 6ê°œ ì„¤ì • í•„ë“œ, ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜, DB íŠ¸ëœì­ì…˜ íŒ¨í„´
- **ë¶€ì •í™•í•œ ë¶€ë¶„**: ë…¸ë“œ ìœ„ì¹˜ Line ë²ˆí˜¸ (ì‹¤ì œ ì½”ë“œì™€ ì•½ê°„ ì°¨ì´)
- **ëˆ„ë½ëœ ë¶€ë¶„**: í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°, LLM í˜¸ì¶œ íŒ¨í„´

#### âš ï¸ ë°œê²¬ëœ ì´ìŠˆ

1. **í”„ë¡¬í”„íŠ¸ íŒŒì¼ ëˆ„ë½**
   - `conversation_summary.txt` íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
   - í˜„ì¬ prompts êµ¬ì¡°: cognitive/, common/, execution/ (3ê°œ ì¹´í…Œê³ ë¦¬ë§Œ)
   - í•„ìš”: common/conversation_summary.txt ì¶”ê°€

2. **Line ë²ˆí˜¸ ë¶ˆì¼ì¹˜**
   - ê³„íšì„œ Line 870 â†’ ì‹¤ì œ Line 174-252 (generate_response_node)
   - ê³„íšì„œ Line 235-263 â†’ ì‹¤ì œì™€ ì¼ì¹˜ (planning_node)

3. **State í•„ë“œ ì •ì˜ ëˆ„ë½**
   - `tiered_memories` í•„ë“œê°€ separated_states.pyì— ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ
   - í•„ìˆ˜ëŠ” ì•„ë‹ˆì§€ë§Œ, íƒ€ì… ì•ˆì •ì„±ì„ ìœ„í•´ ê¶Œì¥

---

## í˜„ì¬ ì½”ë“œ ìƒíƒœ ê²€ì¦

### 1. Config.py (backend/app/core/config.py)

**í˜„ì¬ ìƒíƒœ**:
```python
# Line 1-2
from typing import List
from pydantic_settings import BaseSettings

# Line 31
MEMORY_LOAD_LIMIT: int = 5
```

**ê²€ì¦ ê²°ê³¼**:
- âœ… MEMORY_LOAD_LIMIT ì¡´ì¬
- âŒ `from pydantic import Field` ëˆ„ë½
- âŒ 6ê°œ ì‹ ê·œ Field ë¯¸ì •ì˜

**í•„ìš”í•œ ìˆ˜ì •**:
```python
# Line 2ì— ì¶”ê°€
from pydantic import Field

# Line 31 ì´í›„ ì¶”ê°€
SHORTTERM_MEMORY_LIMIT: int = Field(default=5, description="...")
MIDTERM_MEMORY_LIMIT: int = Field(default=5, description="...")
LONGTERM_MEMORY_LIMIT: int = Field(default=10, description="...")
MEMORY_TOKEN_LIMIT: int = Field(default=2000, description="...")
MEMORY_MESSAGE_LIMIT: int = Field(default=10, description="...")
SUMMARY_MAX_LENGTH: int = Field(default=200, description="...")
```

---

### 2. SimpleMemoryService (simple_memory_service.py)

**í˜„ì¬ ìƒíƒœ (Line 5-10)**:
```python
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy import select, desc  # â† and_ ëˆ„ë½
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm.attributes import flag_modified
```

**ê²€ì¦ ê²°ê³¼**:
- âœ… ê¸°ë³¸ imports ì¡´ì¬
- âŒ `import asyncio` ëˆ„ë½
- âŒ `import tiktoken` ëˆ„ë½
- âŒ `and_` ëˆ„ë½ (from sqlalchemy)

**ì´ ë¼ì¸ ìˆ˜**: 392 lines
**ê¸°ì¡´ ë©”ì„œë“œ**:
- `load_recent_memories()` (Lines 217-329) âœ…
- `save_conversation()` (Lines 331-386) âœ…

**ëˆ„ë½ëœ ë©”ì„œë“œ** (6ê°œ):
1. `load_tiered_memories()` - 3-Tier êµ¬ì¡° ë¡œë“œ
2. `_get_or_create_summary()` - ìš”ì•½ ìºì‹œ ì¡°íšŒ
3. `summarize_with_llm()` - LLM ìš”ì•½ ìƒì„±
4. `_save_summary_to_metadata()` - JSONB ì €ì¥
5. `summarize_conversation_background()` - ë°±ê·¸ë¼ìš´ë“œ ì§„ì…ì 
6. `_background_summary_task()` - ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰

---

### 3. TeamSupervisor (team_supervisor.py)

#### A. planning_node (Lines 174-397)

**í˜„ì¬ ë©”ëª¨ë¦¬ ë¡œë”© ë¡œì§ (Lines 235-263)**:
```python
user_id = state.get("user_id")
chat_session_id = state.get("chat_session_id")
if user_id:
    try:
        logger.info(f"[TeamSupervisor] Loading Long-term Memory for user {user_id}")
        async for db_session in get_async_db():
            memory_service = LongTermMemoryService(db_session)

            # ìµœê·¼ ëŒ€í™” ê¸°ë¡ ë¡œë“œ (RELEVANTë§Œ, í˜„ì¬ ì„¸ì…˜ ì œì™¸)
            loaded_memories = await memory_service.load_recent_memories(
                user_id=user_id,
                limit=settings.MEMORY_LOAD_LIMIT,
                relevance_filter="RELEVANT",
                session_id=chat_session_id  # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì„¸ì…˜ ì œì™¸
            )

            # ì‚¬ìš©ì ì„ í˜¸ë„ ë¡œë“œ
            user_preferences = await memory_service.get_user_preferences(user_id)

            state["loaded_memories"] = loaded_memories
            state["user_preferences"] = user_preferences
            state["memory_load_time"] = datetime.now().isoformat()

            logger.info(f"[TeamSupervisor] Loaded {len(loaded_memories)} memories and preferences for user {user_id}")
            break  # get_db()ëŠ” generatorì´ë¯€ë¡œ ì²« ë²ˆì§¸ ì„¸ì…˜ë§Œ ì‚¬ìš©
    except Exception as e:
        logger.error(f"[TeamSupervisor] Failed to load Long-term Memory: {e}")
```

**ê²€ì¦ ê²°ê³¼**:
- âœ… DB íŠ¸ëœì­ì…˜ íŒ¨í„´ ì •í™• (`async for db_session in get_async_db()`)
- âœ… ì—ëŸ¬ í•¸ë“¤ë§ ì¡´ì¬
- âœ… State ì—…ë°ì´íŠ¸ íŒ¨í„´ ì •í™•
- âš ï¸ `tiered_memories` í•„ë“œ ì¶”ê°€ í•„ìš”

**í•„ìš”í•œ ìˆ˜ì •**:
```python
# load_recent_memories() í˜¸ì¶œì„ load_tiered_memories()ë¡œ êµì²´
tiered_memories = await memory_service.load_tiered_memories(
    user_id=user_id,
    current_session_id=chat_session_id
)

# Stateì— tiered_memories ì¶”ê°€
state["tiered_memories"] = tiered_memories
state["loaded_memories"] = (  # í•˜ìœ„ í˜¸í™˜ì„±
    tiered_memories.get("shortterm", []) +
    tiered_memories.get("midterm", []) +
    tiered_memories.get("longterm", [])
)
```

#### B. generate_response_node (Lines 174-252)

**í˜„ì¬ ë©”ëª¨ë¦¬ ì €ì¥ ë¡œì§ (Lines 216-250)**:
```python
user_id = state.get("user_id")
if user_id and intent_type not in ["irrelevant", "unclear"]:
    try:
        logger.info(f"[TeamSupervisor] Saving conversation to Long-term Memory for user {user_id}")

        async for db_session in get_async_db():
            memory_service = LongTermMemoryService(db_session)

            # ì‘ë‹µ ìš”ì•½ ìƒì„± (ìµœëŒ€ 200ì)
            response_summary = response.get("summary", "")
            if not response_summary and response.get("answer"):
                response_summary = response.get("answer", "")[:200]
            if not response_summary:
                response_summary = f"{response.get('type', 'response')} ìƒì„± ì™„ë£Œ"

            # chat_session_id ì¶”ì¶œ (Chat History & State Endpoints)
            chat_session_id = state.get("chat_session_id")

            # ëŒ€í™” ì €ì¥ (Phase 1: ê°„ì†Œí™”ëœ 4ê°œ íŒŒë¼ë¯¸í„°)
            await memory_service.save_conversation(
                user_id=user_id,
                session_id=chat_session_id,
                messages=[],  # Phase 1ì—ì„œëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸
                summary=response_summary
            )

            logger.info(f"[TeamSupervisor] Conversation saved to Long-term Memory")
            break  # get_db()ëŠ” generatorì´ë¯€ë¡œ ì²« ë²ˆì§¸ ì„¸ì…˜ë§Œ ì‚¬ìš©
    except Exception as e:
        logger.error(f"[TeamSupervisor] Failed to save Long-term Memory: {e}")
```

**ê²€ì¦ ê²°ê³¼**:
- âœ… save_conversation ì‹œê·¸ë‹ˆì²˜ ì •í™• (user_id, session_id, messages, summary)
- âœ… DB íŠ¸ëœì­ì…˜ íŒ¨í„´ ì •í™•
- âœ… ì—ëŸ¬ í•¸ë“¤ë§ ì¡´ì¬
- âš ï¸ ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½ í˜¸ì¶œ ì¶”ê°€ í•„ìš”

**í•„ìš”í•œ ìˆ˜ì •**:
```python
# save_conversation í˜¸ì¶œ ì „ì— ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½ ì‹œì‘
await memory_service.summarize_conversation_background(
    session_id=chat_session_id,
    user_id=user_id,
    messages=[]
)
```

---

### 4. LLMService (llm_service.py)

**complete_async() ë©”ì„œë“œ (Lines 146-196)**:
```python
async def complete_async(
    self,
    prompt_name: str,
    variables: Dict[str, Any] = None,
    model: str = None,
    temperature: float = None,
    max_tokens: int = None,
    response_format: Dict[str, str] = None,
    **kwargs
) -> str:
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt = self.prompt_manager.get(prompt_name, variables or {})

    # ëª¨ë¸ ì„ íƒ
    if model is None:
        model = Config.LLM_DEFAULTS["models"].get(prompt_name, "gpt-4o-mini")

    # íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "model": model,
        "messages": [{"role": "system", "content": prompt}],
        "temperature": temperature or Config.LLM_DEFAULTS["default_params"]["temperature"],
        "max_tokens": max_tokens or Config.LLM_DEFAULTS["default_params"]["max_tokens"],
    }

    if response_format:
        params["response_format"] = response_format

    params.update(kwargs)

    # ë¹„ë™ê¸° LLM í˜¸ì¶œ with ì¬ì‹œë„
    try:
        response = await self._call_async_with_retry(params)
        self._log_call(prompt_name, response)
        return response.choices[0].message.content

    except Exception as e:
        logger.error(f"Async LLM call failed for {prompt_name}: {e}")
        raise
```

**ê²€ì¦ ê²°ê³¼**:
- âœ… PromptManager ì‚¬ìš© (`self.prompt_manager.get(prompt_name, variables)`)
- âœ… ë¹„ë™ê¸° ì¬ì‹œë„ ë¡œì§ ì¡´ì¬
- âœ… ì—ëŸ¬ í•¸ë“¤ë§ ì¡´ì¬
- âœ… `conversation_summary` í”„ë¡¬í”„íŠ¸ í˜¸ì¶œ ê°€ëŠ¥

**í•„ìš”í•œ ì‘ì—…**:
- `conversation_summary.txt` íŒŒì¼ë§Œ ìƒì„±í•˜ë©´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

---

### 5. PromptManager (prompt_manager.py)

**í”„ë¡¬í”„íŠ¸ ë¡œë”© ë¡œì§ (Lines 160-202)**:
```python
def _load_template(self, prompt_name: str, category: str = None) -> str:
    # ìºì‹œ í™•ì¸
    cache_key = f"{category}/{prompt_name}" if category else prompt_name
    if cache_key in self._cache:
        logger.debug(f"Using cached prompt: {cache_key}")
        return self._cache[cache_key]

    # íŒŒì¼ ê²½ë¡œ ê²°ì •
    file_path = self._find_prompt_file(prompt_name, category)

    if not file_path or not file_path.exists():
        raise FileNotFoundError(
            f"Prompt template not found: {prompt_name} "
            f"(category: {category or 'auto'})"
        )

    # íŒŒì¼ ë¡œë“œ
    logger.debug(f"Loading prompt from: {file_path}")

    if file_path.suffix == ".yaml" or file_path.suffix == ".yml":
        template, metadata = self._load_yaml_template(file_path)
        self._metadata_cache[prompt_name] = metadata
    else:  # .txt
        with open(file_path, 'r', encoding='utf-8') as f:
            template = f.read()

    # ìºì‹œ ì €ì¥
    self._cache[cache_key] = template

    return template
```

**ê²€ì¦ ê²°ê³¼**:
- âœ… .txt íŒŒì¼ ìë™ ë¡œë“œ ì§€ì›
- âœ… 3ê°œ ì¹´í…Œê³ ë¦¬ íƒìƒ‰ (cognitive, execution, common)
- âœ… ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ì¡´ì¬

**í•„ìš”í•œ ì‘ì—…**:
- `prompts/common/conversation_summary.txt` íŒŒì¼ë§Œ ìƒì„±í•˜ë©´ ìë™ ë¡œë“œë¨

---

### 6. SeparatedStates (separated_states.py)

**MainSupervisorState ì •ì˜ (Lines 286-332)**:
```python
class MainSupervisorState(TypedDict, total=False):
    # ... ê¸°ì¡´ í•„ë“œë“¤ ...

    # Long-term Memory Fields (Line 329-332)
    user_id: Optional[int]
    loaded_memories: Optional[List[Dict[str, Any]]]
    user_preferences: Optional[Dict[str, Any]]
    memory_load_time: Optional[str]

    # âš ï¸ tiered_memories ëˆ„ë½
```

**ê²€ì¦ ê²°ê³¼**:
- âœ… loaded_memories í•„ë“œ ì¡´ì¬
- âœ… user_id, user_preferences ì¡´ì¬
- âŒ tiered_memories í•„ë“œ ëˆ„ë½

**í•„ìš”í•œ ì¶”ê°€**:
```python
# Line 332 ì´í›„ ì¶”ê°€
tiered_memories: Optional[Dict[str, List[Dict[str, Any]]]]
```

---

## ì˜í–¥ ë°›ëŠ” íŒŒì¼ ì „ì²´ ëª©ë¡

### ì§ì ‘ ìˆ˜ì • í•„ìš” (7ê°œ íŒŒì¼)

| íŒŒì¼ ê²½ë¡œ | ìˆ˜ì • ë‚´ìš© | ì˜ˆìƒ ì‹œê°„ |
|----------|----------|----------|
| `backend/app/core/config.py` | Field import, 6ê°œ ì„¤ì • ì¶”ê°€ | 5ë¶„ |
| `backend/.env` | 6ê°œ í™˜ê²½ë³€ìˆ˜ ì¶”ê°€ | 2ë¶„ |
| `backend/app/service_agent/foundation/simple_memory_service.py` | imports, 6ê°œ ë©”ì„œë“œ ì¶”ê°€ | 80ë¶„ |
| `backend/app/service_agent/supervisor/team_supervisor.py` | planning_node, generate_response_node ìˆ˜ì • | 50ë¶„ |
| `backend/app/service_agent/cognitive_agents/planning_agent.py` | tiered_memories í™œìš© ë¡œì§ | 30ë¶„ |
| `backend/app/service_agent/llm_manager/prompts/common/conversation_summary.txt` | í”„ë¡¬í”„íŠ¸ íŒŒì¼ ìƒì„± | 10ë¶„ |
| `backend/app/service_agent/foundation/separated_states.py` | tiered_memories í•„ë“œ ì¶”ê°€ (ì„ íƒ) | 5ë¶„ |

### ê°„ì ‘ ì˜í–¥ (ìë™ í˜¸í™˜)

| íŒŒì¼ ê²½ë¡œ | ì˜í–¥ ë‚´ìš© | ì¡°ì¹˜ í•„ìš” |
|----------|----------|----------|
| `backend/app/service_agent/llm_manager/llm_service.py` | ìƒˆ í”„ë¡¬í”„íŠ¸ í˜¸ì¶œ | ì—†ìŒ (ìë™) |
| `backend/app/service_agent/llm_manager/prompt_manager.py` | ìƒˆ í”„ë¡¬í”„íŠ¸ ë¡œë“œ | ì—†ìŒ (ìë™) |
| `backend/app/models/chat.py` | JSONB metadata ì‚¬ìš© | ì—†ìŒ (ê¸°ì¡´ êµ¬ì¡°) |
| `backend/app/db/postgre_db.py` | DB ì„¸ì…˜ ì œê³µ | ì—†ìŒ (ê¸°ì¡´ íŒ¨í„´) |

### í…ŒìŠ¤íŠ¸ íŒŒì¼ (1ê°œ)

| íŒŒì¼ ê²½ë¡œ | ëª©ì  | ì˜ˆìƒ ì‹œê°„ |
|----------|------|----------|
| `backend/test_3tier_memory.py` | í†µí•© í…ŒìŠ¤íŠ¸ | 50ë¶„ |

---

## ì½”ë“œ íë¦„ ë¶„ì„

### 1. State ì „íŒŒ íë¦„

```
ì‚¬ìš©ì ìš”ì²­
    â†“
[initialize_node]
    â†“
    state["user_id"] = user_id
    state["chat_session_id"] = session_id
    â†“
[planning_node] â† âš¡ ë©”ëª¨ë¦¬ ë¡œë“œ ì§€ì 
    â†“
    async for db_session in get_async_db():
        memory_service = LongTermMemoryService(db_session)

        # ğŸ”µ í˜„ì¬ (Phase 0)
        loaded_memories = await memory_service.load_recent_memories(
            user_id=user_id,
            limit=settings.MEMORY_LOAD_LIMIT,
            session_id=chat_session_id
        )
        state["loaded_memories"] = loaded_memories

        # ğŸŸ¢ ë³€ê²½ í›„ (Phase 1)
        tiered_memories = await memory_service.load_tiered_memories(
            user_id=user_id,
            current_session_id=chat_session_id
        )
        state["tiered_memories"] = tiered_memories
        state["loaded_memories"] = tiered_memories["shortterm"] + ...
    â†“
[execute_teams_node]
    â†“
    shared_state = StateManager.create_shared_state(
        query=state["query"],
        session_id=state["session_id"]
    )
    # íŒ€ ì‹¤í–‰ (SearchTeam, AnalysisTeam, DocumentTeam)
    â†“
[aggregate_results_node]
    â†“
    state["aggregated_results"] = {...}
    â†“
[generate_response_node] â† âš¡ ë©”ëª¨ë¦¬ ì €ì¥ ì§€ì 
    â†“
    async for db_session in get_async_db():
        memory_service = LongTermMemoryService(db_session)

        # ğŸŸ¢ ì¶”ê°€ (Phase 1)
        await memory_service.summarize_conversation_background(
            session_id=chat_session_id,
            user_id=user_id,
            messages=[]
        )

        # ê¸°ì¡´ (Phase 0)
        await memory_service.save_conversation(
            user_id=user_id,
            session_id=chat_session_id,
            messages=[],
            summary=response_summary
        )
    â†“
    state["final_response"] = response
    â†“
ì‚¬ìš©ì ì‘ë‹µ ë°˜í™˜
```

### 2. ë©”ëª¨ë¦¬ ë¡œë“œ ìƒì„¸ íë¦„

```python
# planning_node (Line 235-263)
user_id = state.get("user_id")  # ì˜ˆ: 1
chat_session_id = state.get("chat_session_id")  # ì˜ˆ: "session-abc-123"

# LongTermMemoryService ì´ˆê¸°í™”
async for db_session in get_async_db():
    memory_service = LongTermMemoryService(db_session)

    # ğŸ”µ Phase 0: ê¸°ì¡´ ë°©ì‹
    loaded_memories = await memory_service.load_recent_memories(
        user_id=1,
        limit=5,  # settings.MEMORY_LOAD_LIMIT
        relevance_filter="RELEVANT",
        session_id="session-abc-123"
    )
    # ë°˜í™˜ í˜•ì‹:
    # [
    #     {"session_id": "session-xyz", "summary": "ê°•ë‚¨êµ¬ ì „ì„¸ ì‹œì„¸ ë¬¸ì˜", ...},
    #     {"session_id": "session-def", "summary": "ëŒ€ì¶œ ì¡°ê±´ í™•ì¸", ...},
    #     ...
    # ]

    # ğŸŸ¢ Phase 1: 3-Tier ë°©ì‹
    tiered_memories = await memory_service.load_tiered_memories(
        user_id=1,
        current_session_id="session-abc-123"
    )
    # ë°˜í™˜ í˜•ì‹:
    # {
    #     "shortterm": [
    #         {
    #             "session_id": "session-1",
    #             "messages": [
    #                 {"role": "user", "content": "...", "timestamp": "..."},
    #                 {"role": "assistant", "content": "...", "timestamp": "..."},
    #                 ...
    #             ],
    #             "metadata": {...},
    #             "tier": "shortterm"
    #         },
    #         ...  # ìµœê·¼ 5ê°œ ì„¸ì…˜ (ì„¤ì • ê°€ëŠ¥)
    #     ],
    #     "midterm": [
    #         {
    #             "session_id": "session-6",
    #             "summary": "ê°•ë‚¨êµ¬ ì•„íŒŒíŠ¸ ì „ì„¸ ì‹œì„¸ ë° ëŒ€ì¶œ ì¡°ê±´ ë¬¸ì˜",
    #             "metadata": {...},
    #             "tier": "midterm"
    #         },
    #         ...  # 6-10ë²ˆì§¸ ì„¸ì…˜ (ì„¤ì • ê°€ëŠ¥)
    #     ],
    #     "longterm": [
    #         {
    #             "session_id": "session-11",
    #             "summary": "ì„œì´ˆêµ¬ ì˜¤í”¼ìŠ¤í…” ì›”ì„¸ ê´€ë ¨ ë²•ë¥  ìƒë‹´",
    #             "metadata": {...},
    #             "tier": "longterm"
    #         },
    #         ...  # 11-20ë²ˆì§¸ ì„¸ì…˜ (ì„¤ì • ê°€ëŠ¥)
    #     ]
    # }

    # State ì—…ë°ì´íŠ¸
    state["tiered_memories"] = tiered_memories
    state["loaded_memories"] = (  # í•˜ìœ„ í˜¸í™˜ì„±
        tiered_memories["shortterm"] +
        tiered_memories["midterm"] +
        tiered_memories["longterm"]
    )
```

### 3. LLM ìš”ì•½ ìƒì„± íë¦„

```python
# generate_response_node (Line 216-250)
async for db_session in get_async_db():
    memory_service = LongTermMemoryService(db_session)

    # ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½ ì‹œì‘ (fire-and-forget)
    await memory_service.summarize_conversation_background(
        session_id="session-abc-123",
        user_id=1,
        messages=[]
    )
    # â†“ (ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
    # asyncio.create_task(_background_summary_task(...))
    #     â†“
    #     summarize_with_llm("session-abc-123")
    #         â†“
    #         1. chat_messagesì—ì„œ ëŒ€í™” ë¡œë“œ
    #         2. ëŒ€í™” í¬ë§·íŒ… (ìµœê·¼ 10ê°œ ë©”ì‹œì§€)
    #         3. LLMService.complete_async(
    #                prompt_name="conversation_summary",
    #                variables={"conversation": "...", "max_length": 200}
    #            )
    #         4. _save_summary_to_metadata(session_id, summary)
    #             â†“
    #             session.session_metadata["conversation_summary"] = summary
    #             flag_modified(session, "session_metadata")
    #             await db.commit()

    # ê¸°ì¡´ ì €ì¥ ë¡œì§ (ì¦‰ì‹œ ì‹¤í–‰)
    await memory_service.save_conversation(
        user_id=1,
        session_id="session-abc-123",
        messages=[],
        summary=response_summary
    )
```

### 4. í† í° ì œí•œ ë¡œì§

```python
# simple_memory_service.py - load_tiered_memories()
encoding = tiktoken.get_encoding("cl100k_base")
total_tokens = 0

for idx, session in enumerate(sessions):
    # í† í° ì œí•œ ì²´í¬
    if total_tokens >= settings.MEMORY_TOKEN_LIMIT:  # ê¸°ë³¸ 2000
        logger.info(f"Token limit reached: {total_tokens}")
        break

    if idx < settings.SHORTTERM_MEMORY_LIMIT:
        # Short-term: ì „ì²´ ë©”ì‹œì§€
        content_text = " ".join([m["content"] for m in messages])
        tokens = len(encoding.encode(content_text))
        total_tokens += tokens

        if total_tokens > settings.MEMORY_TOKEN_LIMIT:
            break  # ì œí•œ ì´ˆê³¼

    elif idx < settings.SHORTTERM_MEMORY_LIMIT + settings.MIDTERM_MEMORY_LIMIT:
        # Mid-term: ìš”ì•½ë§Œ
        summary = await self._get_or_create_summary(session)
        tokens = len(encoding.encode(summary))
        total_tokens += tokens

    else:
        # Long-term: ìš”ì•½ë§Œ
        summary = await self._get_or_create_summary(session)
        tokens = len(encoding.encode(summary))
        total_tokens += tokens
```

---

## êµ¬í˜„ ê³„íš ê²€ì¦

### Phaseë³„ ì •í™•ë„ í‰ê°€

| Phase | ë‚´ìš© | ì •í™•ë„ | ë°œê²¬ëœ ì´ìŠˆ | ê¶Œì¥ì‚¬í•­ |
|-------|------|--------|------------|----------|
| Phase 1 | ì„¤ì • íŒŒì¼ | âœ… 100% | ì—†ìŒ | ê·¸ëŒ€ë¡œ ì§„í–‰ |
| Phase 2 | ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ | âœ… 98% | import ìˆœì„œ ê¶Œì¥ì‚¬í•­ | Line 387 ì´í›„ ì¶”ê°€ ì •í™• |
| Phase 3 | Supervisor | âš ï¸ 90% | Line ë²ˆí˜¸ ë¶ˆì¼ì¹˜ | ì‹¤ì œ ìœ„ì¹˜: Line 235-263, 174-252 |
| Phase 4 | Planning Agent | âœ… 95% | êµ¬ì²´ì  ìœ„ì¹˜ ëˆ„ë½ | analyze_intent ë©”ì„œë“œ ë‚´ë¶€ |
| Phase 5 | í”„ë¡¬í”„íŠ¸ | âœ… 100% | ì—†ìŒ | common/ ë””ë ‰í† ë¦¬ í™•ì¸ë¨ |
| Phase 6 | í…ŒìŠ¤íŠ¸ | âœ… 100% | ì—†ìŒ | pytest-asyncio í™•ì¸ í•„ìš” |

### ìˆ˜ì •ëœ Line ë²ˆí˜¸

| ê³„íšì„œ | ì‹¤ì œ | ë©”ì„œë“œ | íŒŒì¼ |
|-------|------|--------|------|
| Line 235-263 | âœ… Line 235-263 | planning_node | team_supervisor.py |
| Line 870~ | âŒ Line 174-252 | generate_response_node | team_supervisor.py |

---

## ì ì¬ì  ì´ìŠˆ ë° í•´ê²°ë°©ì•ˆ

### 1. í”„ë¡¬í”„íŠ¸ íŒŒì¼ ëˆ„ë½

**ì´ìŠˆ**:
```bash
FileNotFoundError: Prompt template not found: conversation_summary
```

**ì›ì¸**:
- `backend/app/service_agent/llm_manager/prompts/common/conversation_summary.txt` íŒŒì¼ ì—†ìŒ

**í•´ê²°ë°©ì•ˆ**:
```bash
# íŒŒì¼ ìƒì„±
touch backend/app/service_agent/llm_manager/prompts/common/conversation_summary.txt

# ë‚´ìš© ì‘ì„± (ê³„íšì„œ Phase 5-1 ì°¸ì¡°)
cat > backend/app/service_agent/llm_manager/prompts/common/conversation_summary.txt << 'EOF'
ë‹¹ì‹ ì€ ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ëŒ€í™”ë¥¼ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{conversation}

ìš”ì•½ ê·œì¹™:
1. í•µì‹¬ ì£¼ì œì™€ ê²°ë¡ ë§Œ í¬í•¨
2. ì‚¬ìš©ìì˜ ì£¼ìš” ìš”êµ¬ì‚¬í•­ ëª…ì‹œ
3. ì¤‘ìš”í•œ ê²°ì •ì‚¬í•­ì´ë‚˜ í•©ì˜ ë‚´ìš© í¬í•¨
4. ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§ì´ë‚˜ ë°˜ë³µ ì œì™¸

ìš”ì•½:
EOF
```

---

### 2. tiktoken ì„¤ì¹˜ í™•ì¸

**ì´ìŠˆ**:
```python
ModuleNotFoundError: No module named 'tiktoken'
```

**í•´ê²°ë°©ì•ˆ**:
```bash
# ì„¤ì¹˜
pip install tiktoken

# ë˜ëŠ” requirements.txtì— ì¶”ê°€
echo "tiktoken==0.5.2" >> backend/requirements.txt
pip install -r backend/requirements.txt
```

---

### 3. DB ì„¸ì…˜ íƒ€ì´ë° ì´ìŠˆ

**ì ì¬ì  ë¬¸ì œ**:
- ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ì—ì„œ DB ì„¸ì…˜ì´ ì´ë¯¸ ë‹«íŒ ê²½ìš°

**í˜„ì¬ ì½”ë“œ**:
```python
# generate_response_node
async for db_session in get_async_db():
    memory_service = LongTermMemoryService(db_session)

    # ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘
    await memory_service.summarize_conversation_background(...)

    # ì¦‰ì‹œ ì €ì¥
    await memory_service.save_conversation(...)

    break  # â† DB ì„¸ì…˜ ì¢…ë£Œ
```

**ë¬¸ì œ**:
- `summarize_conversation_background()`ê°€ `asyncio.create_task()`ë¡œ ì‹¤í–‰
- ë©”ì¸ í”Œë¡œìš°ê°€ `break`ë¡œ ì¢…ë£Œë˜ë©´ db_session ë‹«í˜
- ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ì—ì„œ `_save_summary_to_metadata()`ê°€ ë‹«íŒ ì„¸ì…˜ ì‚¬ìš©

**í•´ê²°ë°©ì•ˆ 1**: ë…ë¦½ ì„¸ì…˜ ìƒì„±
```python
async def _background_summary_task(self, session_id: str, user_id: int, messages: List) -> None:
    try:
        # ìƒˆë¡œìš´ DB ì„¸ì…˜ ìƒì„±
        async for db_session in get_async_db():
            memory_service = LongTermMemoryService(db_session)

            summary = await memory_service.summarize_with_llm(session_id)
            await memory_service._save_summary_to_metadata(session_id, summary)

            break
    except Exception as e:
        logger.error(f"Background summary failed: {e}")
```

**í•´ê²°ë°©ì•ˆ 2**: ì„¸ì…˜ IDë§Œ ì „ë‹¬
```python
async def summarize_conversation_background(
    self,
    session_id: str,
    user_id: int,
    messages: List[Dict[str, Any]]
) -> None:
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëŒ€í™” ìš”ì•½ (ì„¸ì…˜ ë…ë¦½)"""
    asyncio.create_task(
        self._background_summary_with_new_session(session_id, user_id)
    )

async def _background_summary_with_new_session(self, session_id: str, user_id: int) -> None:
    """ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½"""
    try:
        async for db_session in get_async_db():
            temp_service = LongTermMemoryService(db_session)
            summary = await temp_service.summarize_with_llm(session_id)
            await temp_service._save_summary_to_metadata(session_id, summary)
            break
    except Exception as e:
        logger.error(f"Background summary failed: {e}")
```

**ê¶Œì¥**: í•´ê²°ë°©ì•ˆ 2 (ë…ë¦½ ì„¸ì…˜)

---

### 4. State íƒ€ì… ì•ˆì „ì„±

**ì´ìŠˆ**:
- `tiered_memories` í•„ë“œê°€ TypedDictì— ì •ì˜ë˜ì§€ ì•Šìœ¼ë©´ IDE ê²½ê³ 

**í˜„ì¬**:
```python
state["tiered_memories"] = tiered_memories  # â† Type warning
```

**í•´ê²°ë°©ì•ˆ**:
```python
# separated_states.py Line 332 ì´í›„ ì¶”ê°€
class MainSupervisorState(TypedDict, total=False):
    # ... ê¸°ì¡´ í•„ë“œë“¤ ...

    # Long-term Memory Fields
    user_id: Optional[int]
    loaded_memories: Optional[List[Dict[str, Any]]]
    user_preferences: Optional[Dict[str, Any]]
    memory_load_time: Optional[str]

    # 3-Tier Memory (Phase 1)
    tiered_memories: Optional[Dict[str, List[Dict[str, Any]]]]  # â† ì¶”ê°€
```

---

### 5. Planning Agent í†µí•© ìœ„ì¹˜

**ì´ìŠˆ**:
- ê³„íšì„œì— "ì ì ˆí•œ ë©”ì„œë“œ ë‚´ë¶€"ë¼ê³ ë§Œ ëª…ì‹œë¨

**ì •í™•í•œ ìœ„ì¹˜**:
```python
# planning_agent.py - analyze_intent ë©”ì„œë“œ
async def _analyze_with_llm(self, query: str, context: Optional[Dict]) -> IntentResult:
    """LLMì„ ì‚¬ìš©í•œ ì˜ë„ ë¶„ì„"""
    try:
        # Contextì—ì„œ chat_history ì¶”ì¶œ
        chat_history = context.get("chat_history", []) if context else []

        # ğŸŸ¢ ì—¬ê¸°ì— tiered_memories í†µí•© ì¶”ê°€
        tiered_memories = context.get("tiered_memories", {}) if context else {}

        memory_context = ""
        if tiered_memories:
            # Short-term: ì „ì²´ ëŒ€í™”
            if tiered_memories.get("shortterm"):
                memory_context += "=== ìµœê·¼ ëŒ€í™” (ì „ì²´) ===\n"
                for mem in tiered_memories["shortterm"]:
                    for msg in mem.get("messages", []):
                        memory_context += f"{msg['role']}: {msg['content'][:100]}...\n"
                memory_context += "\n"

            # Mid-term: ìš”ì•½
            if tiered_memories.get("midterm"):
                memory_context += "=== ì¤‘ê¸° ëŒ€í™” (ìš”ì•½) ===\n"
                for mem in tiered_memories["midterm"]:
                    memory_context += f"- {mem.get('summary', '')}\n"
                memory_context += "\n"

            # Long-term: ìš”ì•½
            if tiered_memories.get("longterm"):
                memory_context += "=== ì¥ê¸° ëŒ€í™” (ìš”ì•½) ===\n"
                for mem in tiered_memories["longterm"]:
                    memory_context += f"- {mem.get('summary', '')}\n"

        # LLMServiceë¥¼ í†µí•œ ì˜ë„ ë¶„ì„
        result = await self.llm_service.complete_json_async(
            prompt_name="intent_analysis",
            variables={
                "query": query,
                "chat_history": chat_history_text,
                "memory_context": memory_context  # â† ì¶”ê°€
            },
            temperature=0.0,
            max_tokens=500
        )

        # ... ë‚˜ë¨¸ì§€ ì½”ë“œ ...
```

**ì¶”ê°€ í•„ìš” ì‘ì—…**:
- `intent_analysis.txt` í”„ë¡¬í”„íŠ¸ì— `{memory_context}` ë³€ìˆ˜ ì¶”ê°€

---

## ì¢…í•© ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì „ ì¤€ë¹„ (10ë¶„)

- [ ] **ë°±ì—… ìƒì„±**
  ```bash
  cp backend/app/core/config.py backend/app/core/config.py.backup
  cp backend/app/service_agent/foundation/simple_memory_service.py backend/app/service_agent/foundation/simple_memory_service.py.backup
  cp backend/app/service_agent/supervisor/team_supervisor.py backend/app/service_agent/supervisor/team_supervisor.py.backup
  cp backend/.env backend/.env.backup
  ```

- [ ] **ì˜ì¡´ì„± í™•ì¸ ë° ì„¤ì¹˜**
  ```bash
  pip install tiktoken pytest-asyncio
  ```

- [ ] **í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸**
  ```bash
  ls backend/app/service_agent/llm_manager/prompts/common/
  # ì¶œë ¥ì— conversation_summary.txtê°€ ì—†ìœ¼ë©´ ìƒì„± í•„ìš”
  ```

---

### Phase 1: ì„¤ì • (15ë¶„)

- [ ] **config.py - Line 2 ìˆ˜ì •**
  ```python
  from pydantic import Field  # â† ì¶”ê°€
  ```

- [ ] **config.py - Line 31 ì´í›„ ì¶”ê°€**
  ```python
  SHORTTERM_MEMORY_LIMIT: int = Field(default=5, description="ìµœê·¼ Nê°œ ì„¸ì…˜ ì „ì²´ ë©”ì‹œì§€ ë¡œë“œ")
  MIDTERM_MEMORY_LIMIT: int = Field(default=5, description="ì¤‘ê¸° ë©”ëª¨ë¦¬ ì„¸ì…˜ ìˆ˜ (6-10ë²ˆì§¸)")
  LONGTERM_MEMORY_LIMIT: int = Field(default=10, description="ì¥ê¸° ë©”ëª¨ë¦¬ ì„¸ì…˜ ìˆ˜ (11-20ë²ˆì§¸)")
  MEMORY_TOKEN_LIMIT: int = Field(default=2000, description="ë©”ëª¨ë¦¬ ë¡œë“œ ì‹œ ìµœëŒ€ í† í° ì œí•œ")
  MEMORY_MESSAGE_LIMIT: int = Field(default=10, description="Short-term ì„¸ì…˜ë‹¹ ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜")
  SUMMARY_MAX_LENGTH: int = Field(default=200, description="LLM ìš”ì•½ ìµœëŒ€ ê¸€ì ìˆ˜")
  ```

- [ ] **.env - í™˜ê²½ë³€ìˆ˜ ì¶”ê°€**
  ```bash
  SHORTTERM_MEMORY_LIMIT=5
  MIDTERM_MEMORY_LIMIT=5
  LONGTERM_MEMORY_LIMIT=10
  MEMORY_TOKEN_LIMIT=2000
  MEMORY_MESSAGE_LIMIT=10
  SUMMARY_MAX_LENGTH=200
  ```

- [ ] **ì„¤ì • ê²€ì¦**
  ```bash
  python -c "from app.core.config import settings; print(settings.SHORTTERM_MEMORY_LIMIT)"
  # ì¶œë ¥: 5
  ```

---

### Phase 2: ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ (1ì‹œê°„ 20ë¶„)

- [ ] **simple_memory_service.py - imports ìˆ˜ì • (Line 5-10)**
  ```python
  import logging
  from typing import List, Dict, Any, Optional
  from datetime import datetime
  import asyncio  # â† ì¶”ê°€
  import tiktoken  # â† ì¶”ê°€
  from sqlalchemy import select, desc, and_  # â† and_ ì¶”ê°€
  ```

- [ ] **Line 387 ì´í›„ - load_tiered_memories() ì¶”ê°€**
  - ê³„íšì„œ Phase 2-2-A ì „ì²´ ì½”ë“œ ë³µì‚¬

- [ ] **_get_or_create_summary() ì¶”ê°€**
  - ê³„íšì„œ Phase 2-2-B ì „ì²´ ì½”ë“œ ë³µì‚¬

- [ ] **summarize_with_llm() ì¶”ê°€**
  - ê³„íšì„œ Phase 2-2-C ì „ì²´ ì½”ë“œ ë³µì‚¬

- [ ] **_save_summary_to_metadata() ì¶”ê°€**
  - ê³„íšì„œ Phase 2-2-D ì „ì²´ ì½”ë“œ ë³µì‚¬

- [ ] **summarize_conversation_background() ì¶”ê°€**
  - âš ï¸ ìˆ˜ì •ëœ ë²„ì „ (ë…ë¦½ ì„¸ì…˜) ì‚¬ìš©:
  ```python
  async def summarize_conversation_background(
      self,
      session_id: str,
      user_id: int,
      messages: List[Dict[str, Any]]
  ) -> None:
      """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëŒ€í™” ìš”ì•½ (ë…ë¦½ ì„¸ì…˜)"""
      asyncio.create_task(
          self._background_summary_with_new_session(session_id, user_id)
      )
  ```

- [ ] **_background_summary_with_new_session() ì¶”ê°€**
  - âš ï¸ ìƒˆ ë²„ì „ (ë…ë¦½ ì„¸ì…˜):
  ```python
  async def _background_summary_with_new_session(
      self,
      session_id: str,
      user_id: int
  ) -> None:
      """ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½"""
      try:
          async for db_session in get_async_db():
              temp_service = LongTermMemoryService(db_session)
              summary = await temp_service.summarize_with_llm(session_id)
              await temp_service._save_summary_to_metadata(session_id, summary)
              break
      except Exception as e:
          logger.error(f"Background summary failed: {e}")
  ```

- [ ] **ë©”ì„œë“œ ì¡´ì¬ í™•ì¸**
  ```python
  from app.service_agent.foundation.simple_memory_service import SimpleMemoryService
  print(hasattr(SimpleMemoryService, 'load_tiered_memories'))  # True
  print(hasattr(SimpleMemoryService, 'summarize_with_llm'))  # True
  ```

---

### Phase 3: Supervisor (50ë¶„)

- [ ] **team_supervisor.py - planning_node ìˆ˜ì • (Line 235-263)**
  - âš ï¸ ì •í™•í•œ ìœ„ì¹˜: Line 244-249 (load_recent_memories í˜¸ì¶œ ë¶€ë¶„)
  - ê³„íšì„œ Phase 3-1 ìˆ˜ì • í›„ ì½”ë“œ ì‚¬ìš©

- [ ] **team_supervisor.py - generate_response_node ìˆ˜ì •**
  - âš ï¸ ì •í™•í•œ ìœ„ì¹˜: Line 216-250 (save_conversation í˜¸ì¶œ ë¶€ë¶„)
  - ê³„íšì„œ Phase 3-2 ìˆ˜ì • í›„ ì½”ë“œ ì‚¬ìš©
  - ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½ í˜¸ì¶œ ì¶”ê°€

- [ ] **ë¡œê·¸ í™•ì¸**
  ```bash
  # ì„œë²„ ì‹¤í–‰ í›„ í…ŒìŠ¤íŠ¸ ìš”ì²­
  tail -f backend/logs/app.log | grep "3-Tier"
  # ì¶œë ¥ ì˜ˆìƒ: "3-Tier memories loaded - Short(5), Mid(3), Long(2)"
  ```

---

### Phase 4: Planning Agent (30ë¶„)

- [ ] **planning_agent.py - _analyze_with_llm ë©”ì„œë“œ ìˆ˜ì •**
  - ìœ„ì¹˜: Line 183-248 (async def _analyze_with_llm)
  - contextì—ì„œ tiered_memories ì¶”ì¶œ
  - memory_context ë¬¸ìì—´ ìƒì„±
  - variablesì— memory_context ì¶”ê°€

- [ ] **intent_analysis.txt í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (ì„ íƒ)**
  - íŒŒì¼: `backend/app/service_agent/llm_manager/prompts/cognitive/intent_analysis.txt`
  - `{memory_context}` ë³€ìˆ˜ ì¶”ê°€ (í•„ìš” ì‹œ)

---

### Phase 5: í”„ë¡¬í”„íŠ¸ (20ë¶„)

- [ ] **conversation_summary.txt ìƒì„±**
  - ê²½ë¡œ: `backend/app/service_agent/llm_manager/prompts/common/conversation_summary.txt`
  - ë‚´ìš©: ê³„íšì„œ Phase 5-1 ì „ì²´ ë³µì‚¬

- [ ] **í”„ë¡¬í”„íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸**
  ```python
  from app.service_agent.llm_manager.prompt_manager import PromptManager
  pm = PromptManager()
  prompt = pm.get('conversation_summary', {
      'conversation': 'í…ŒìŠ¤íŠ¸ ëŒ€í™”',
      'max_length': 100
  })
  print("SUCCESS" if "í…ŒìŠ¤íŠ¸ ëŒ€í™”" in prompt else "FAILED")
  ```

---

### Phase 6: í…ŒìŠ¤íŠ¸ (50ë¶„)

- [ ] **test_3tier_memory.py ìƒì„±**
  - ê²½ë¡œ: `backend/test_3tier_memory.py`
  - ë‚´ìš©: ê³„íšì„œ Phase 6-1 ì „ì²´ ë³µì‚¬

- [ ] **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**
  ```bash
  # ê°œë³„ í…ŒìŠ¤íŠ¸
  pytest backend/test_3tier_memory.py::test_3tier_memory_loading -v
  pytest backend/test_3tier_memory.py::test_llm_summarization -v

  # ì „ì²´ í…ŒìŠ¤íŠ¸
  pytest backend/test_3tier_memory.py -v
  ```

- [ ] **í†µí•© í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ìš”ì²­)**
  ```bash
  # 1. ì„œë²„ ì‹œì‘
  cd backend
  python main.py

  # 2. ìš”ì²­ ì „ì†¡ (ë³„ë„ í„°ë¯¸ë„)
  curl -X POST http://localhost:8000/api/chat/query \
    -H "Content-Type: application/json" \
    -d '{"query": "ê°•ë‚¨êµ¬ ì•„íŒŒíŠ¸ ì „ì„¸ ì‹œì„¸ ì•Œë ¤ì£¼ì„¸ìš”", "user_id": 1, "session_id": "test-001"}'

  # 3. ë¡œê·¸ í™•ì¸
  tail -f backend/logs/app.log | grep "3-Tier\|tiered_memories"
  ```

---

### ì„ íƒ: State ì •ì˜ (5ë¶„)

- [ ] **separated_states.py - tiered_memories í•„ë“œ ì¶”ê°€**
  - ìœ„ì¹˜: Line 332 ì´í›„
  ```python
  tiered_memories: Optional[Dict[str, List[Dict[str, Any]]]]
  ```

- [ ] **íƒ€ì… ê²€ì¦**
  ```python
  from app.service_agent.foundation.separated_states import MainSupervisorState
  import inspect
  print("tiered_memories" in MainSupervisorState.__annotations__)  # True
  ```

---

### ìµœì¢… ê²€ì¦ (15ë¶„)

- [ ] **ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸**
  1. ì‚¬ìš©ì ìš”ì²­ â†’ planning_nodeì—ì„œ ë©”ëª¨ë¦¬ ë¡œë“œ í™•ì¸
  2. generate_response_nodeì—ì„œ ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½ ì‹œì‘ í™•ì¸
  3. DBì— conversation_summary ì €ì¥ í™•ì¸
  4. ë‹¤ìŒ ìš”ì²­ì—ì„œ 3-Tier êµ¬ì¡°ë¡œ ë¡œë“œ í™•ì¸

- [ ] **DB í™•ì¸**
  ```sql
  -- PostgreSQL
  SELECT
      session_id,
      session_metadata->'conversation_summary' as summary,
      session_metadata->'summary_method' as method,
      session_metadata->'summary_updated_at' as updated
  FROM chat_sessions
  WHERE user_id = 1
  ORDER BY updated_at DESC
  LIMIT 5;
  ```

- [ ] **ì„±ëŠ¥ ê²€ì¦**
  - í† í° ì œí•œ í™•ì¸ (2000 ì´í•˜)
  - ì‘ë‹µ ì‹œê°„ í™•ì¸ (ë°±ê·¸ë¼ìš´ë“œ ìš”ì•½ìœ¼ë¡œ ì¸í•œ ì§€ì—° ì—†ìŒ)
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­

### 1. êµ¬í˜„ ìˆœì„œ (ìˆœì„œëŒ€ë¡œ í•„ìˆ˜)

1. **Phase 1 â†’ Phase 5** ë¨¼ì € ì™„ë£Œ (ì„¤ì • + í”„ë¡¬í”„íŠ¸)
2. **Phase 2** ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ êµ¬í˜„
3. **Phase 3** Supervisor í†µí•©
4. **Phase 6** í…ŒìŠ¤íŠ¸ ì‹¤í–‰
5. **Phase 4** Planning Agent (ë§ˆì§€ë§‰, ì„ íƒì )

### 2. ì£¼ì˜ì‚¬í•­

âš ï¸ **DB ì„¸ì…˜ ì´ìŠˆ í•´ê²° í•„ìˆ˜**
- ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ëŠ” ë…ë¦½ ì„¸ì…˜ ì‚¬ìš© (`_background_summary_with_new_session`)

âš ï¸ **Line ë²ˆí˜¸ ì •í™•íˆ í™•ì¸**
- ê³„íšì„œ Line 870 â†’ ì‹¤ì œ Line 174-252

âš ï¸ **í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¨¼ì € ìƒì„±**
- `conversation_summary.txt` ì—†ìœ¼ë©´ LLM í˜¸ì¶œ ì‹¤íŒ¨

### 3. í…ŒìŠ¤íŠ¸ ìš°ì„ 

âœ… ê° Phase ì™„ë£Œ í›„ ì¦‰ì‹œ ê²€ì¦
âœ… Phase 2 ì™„ë£Œ â†’ Python REPLì—ì„œ ë©”ì„œë“œ í™•ì¸
âœ… Phase 3 ì™„ë£Œ â†’ ë¡œê·¸ë¡œ 3-Tier ë¡œë“œ í™•ì¸
âœ… Phase 5 ì™„ë£Œ â†’ í”„ë¡¬í”„íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸

### 4. ë¡¤ë°± ì¤€ë¹„

```bash
# ë¬¸ì œ ë°œìƒ ì‹œ ì¦‰ì‹œ ë³µêµ¬
cp backend/app/core/config.py.backup backend/app/core/config.py
cp backend/app/service_agent/foundation/simple_memory_service.py.backup backend/app/service_agent/foundation/simple_memory_service.py
cp backend/app/service_agent/supervisor/team_supervisor.py.backup backend/app/service_agent/supervisor/team_supervisor.py
cp backend/.env.backup backend/.env

# ì„œë²„ ì¬ì‹œì‘
cd backend
python main.py
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### ì„±ê³µ ì‹œ

```log
[TeamSupervisor] Loading 3-Tier Memory for user 1
[SimpleMemoryService] Loaded tiered memories - Tokens: 1847, Short: 5, Mid: 3, Long: 2
[TeamSupervisor] 3-Tier memories loaded - Short(5), Mid(3), Long(2)
[TeamSupervisor] Saving conversation to Long-term Memory for user 1
[SimpleMemoryService] Background summary started for session: session-abc-123
[SimpleMemoryService] LLM summarization completed: ê°•ë‚¨êµ¬ ì•„íŒŒíŠ¸ ì „ì„¸ ì‹œì„¸ ë° ëŒ€ì¶œ ì¡°ê±´ ë¬¸ì˜
[SimpleMemoryService] Summary saved for session: session-abc-123
```

### DB ìƒíƒœ

```sql
-- chat_sessions.session_metadata
{
  "conversation_summary": "ê°•ë‚¨êµ¬ ì•„íŒŒíŠ¸ ì „ì„¸ ì‹œì„¸ ë° ëŒ€ì¶œ ì¡°ê±´ ë¬¸ì˜",
  "summary_method": "llm",
  "summary_updated_at": "2025-10-21T14:30:00",
  "last_updated": "2025-10-21T14:30:00",
  "message_count": 6
}
```

---

**ë³´ê³ ì„œ ì‘ì„± ì™„ë£Œ**
**ë‹¤ìŒ ë‹¨ê³„**: Phase 1ë¶€í„° ì²´í¬ë¦¬ìŠ¤íŠ¸ì— ë”°ë¼ ìˆœì°¨ êµ¬í˜„
