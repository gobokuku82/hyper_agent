# ğŸš€ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ê³ ë„í™” ê³„íš
> ì‘ì„±ì¼: 2025-10-23
> ì‘ì„±ì: AI Assistant
> í˜„ì¬ ìƒíƒœ: ë°ì´í„° ì¬ì‚¬ìš© 85%, 3-Tier Memory 70%, Checkpointer 90%

## ğŸ“Š í˜„ì¬ êµ¬í˜„ í˜„í™© ìš”ì•½

### âœ… ì™„ì„±ëœ ê¸°ëŠ¥ (What Works)
1. **ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì¬ì‚¬ìš©**
   - 41ê°œ í‚¤ì›Œë“œ + 5ê°œ ê°ì§€ ì „ëµ
   - 80% ê°ì§€ìœ¨, 60% SearchTeam ê±´ë„ˆë›°ê¸°
   - í‰ê·  1.3ì´ˆ ì‘ë‹µì‹œê°„ ë‹¨ì¶•

2. **3-Tier Hybrid Memory**
   - Short/Mid/Long-term ê³„ì¸µ êµ¬ì¡°
   - chat_sessions.session_metadata í™œìš©
   - LLM ìš”ì•½ ìë™ ìƒì„±

3. **Checkpointer í†µí•©**
   - AsyncPostgresSaverë¡œ ìƒíƒœ ì˜ì†í™”
   - ì¤‘ë‹¨ëœ ì›Œí¬í”Œë¡œìš° ë³µì› ê°€ëŠ¥

### âŒ ë¯¸êµ¬í˜„/ë¬¸ì œì  (What's Missing)
1. ì „ìš© Memory í…Œì´ë¸” ë¶€ì¬
2. FAISS ë²¡í„° DB ë¯¸í†µí•©
3. ì—”í‹°í‹°/ì„ í˜¸ë„ ì¶”ì  ì—†ìŒ
4. ì„¸ì…˜ ê°„ ê²©ë¦¬ ì œí•œì 

---

## ğŸ¯ ë‹¨ê³„ë³„ ê³ ë„í™” ë¡œë“œë§µ

### Phase 1: ì¦‰ì‹œ ê°œì„  (1ì£¼)
**ëª©í‘œ: í˜„ì¬ ì‹œìŠ¤í…œ ìµœì í™” ë° ì•ˆì •í™”**

#### 1.1 ë°ì´í„° ê°ì§€ ê°œì„ 
```python
# backend/app/service_agent/supervisor/team_supervisor.py

class DataDetector:
    """ì „ìš© ë°ì´í„° ê°ì§€ í´ë˜ìŠ¤"""

    def __init__(self):
        self.structural_patterns = ["##", "**", "â†’", "â€¢", "ğŸ“‹"]
        self.domain_keywords = {
            "legal": ["ë²•ë¥ ", "ê³„ì•½", "ì„ëŒ€", "ê¶Œë¦¬", ...],
            "market": ["ì‹œì„¸", "ë§¤ë§¤", "ì „ì„¸", "ê°€ê²©", ...],
            "real_estate": ["ì•„íŒŒíŠ¸", "ë§¤ë¬¼", "í‰í˜•", ...],
            "analysis": ["ë¶„ì„", "í‰ê°€", "ì¶”ì²œ", ...]
        }
        self.min_content_length = 500

    def detect(self, message: Dict) -> DataDetectionResult:
        """ë‹¤ì¤‘ ì „ëµ ê¸°ë°˜ ê°ì§€"""
        score = 0.0
        reasons = []

        # ì „ëµë³„ ê°€ì¤‘ì¹˜ ì ìš©
        if self._has_structural_patterns(message):
            score += 0.4
            reasons.append("structured_data")

        if self._has_domain_keywords(message):
            score += 0.3
            reasons.append("domain_match")

        if self._has_sufficient_length(message):
            score += 0.2
            reasons.append("substantial_content")

        if self._has_json_structure(message):
            score += 0.1
            reasons.append("json_data")

        return DataDetectionResult(
            has_data=score >= 0.5,
            confidence=score,
            reasons=reasons,
            data_type=self._classify_data_type(message)
        )
```

#### 1.2 ë©”íŠ¸ë¦­ ë¡œê¹… ê°•í™”
```python
# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
class MemoryMetrics:
    def __init__(self):
        self.reuse_attempts = 0
        self.reuse_successes = 0
        self.search_skips = 0
        self.avg_detection_time = 0.0

    def log_reuse_attempt(self, success: bool, detection_time: float):
        self.reuse_attempts += 1
        if success:
            self.reuse_successes += 1
            self.search_skips += 1
        self.avg_detection_time = ...

    def get_stats(self) -> Dict:
        return {
            "reuse_rate": self.reuse_successes / max(self.reuse_attempts, 1),
            "skip_rate": self.search_skips / max(self.reuse_attempts, 1),
            "avg_detection_ms": self.avg_detection_time * 1000
        }
```

#### 1.3 Checkpointer ë°ì´í„° í™œìš©
```python
async def extract_from_checkpoint(
    self,
    thread_id: str,
    data_type: str = "team_results"
) -> Optional[Dict]:
    """Checkpointì—ì„œ ì´ì „ ì‹¤í–‰ ê²°ê³¼ ì¶”ì¶œ"""
    if not self.checkpointer:
        return None

    try:
        # ìµœì‹  checkpoint ì¡°íšŒ
        checkpoint = await self.checkpointer.get_latest(thread_id)
        if checkpoint and checkpoint.get("state"):
            state = checkpoint["state"]

            # team_results ì¶”ì¶œ
            if data_type == "team_results":
                return state.get("team_results", {})

            # aggregated_results ì¶”ì¶œ
            elif data_type == "aggregated":
                return state.get("aggregated_results", {})

        return None
    except Exception as e:
        logger.error(f"Failed to extract from checkpoint: {e}")
        return None
```

---

### Phase 2: ë©”ëª¨ë¦¬ í…Œì´ë¸” êµ¬ì¶• (2ì£¼)

**ëª©í‘œ: ì „ìš© ë©”ëª¨ë¦¬ ì €ì¥ì†Œ êµ¬ì¶•**

#### 2.1 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ
```sql
-- ëŒ€í™” ë©”ëª¨ë¦¬ (ì„¸ì…˜ ë‹¨ìœ„)
CREATE TABLE conversation_memories (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    session_id VARCHAR(100) NOT NULL,
    summary TEXT NOT NULL,
    summary_method VARCHAR(20) DEFAULT 'llm',
    keywords TEXT[],
    entities JSONB,
    importance_score FLOAT DEFAULT 0.5,
    access_count INTEGER DEFAULT 0,
    last_accessed TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),

    INDEX idx_user_session (user_id, session_id),
    INDEX idx_importance (importance_score DESC),
    INDEX idx_keywords (keywords) USING GIN
);

-- ì—”í‹°í‹° ë©”ëª¨ë¦¬ (ê°œì²´ ì¶”ì )
CREATE TABLE entity_memories (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    entity_type VARCHAR(50) NOT NULL, -- 'location', 'property', 'person'
    entity_name VARCHAR(200) NOT NULL,
    properties JSONB NOT NULL,
    first_mentioned TIMESTAMP DEFAULT NOW(),
    last_mentioned TIMESTAMP DEFAULT NOW(),
    mention_count INTEGER DEFAULT 1,

    UNIQUE KEY unique_entity (user_id, entity_type, entity_name),
    INDEX idx_user_type (user_id, entity_type)
);

-- ì‚¬ìš©ì ì„ í˜¸ë„
CREATE TABLE user_preferences (
    user_id INTEGER PRIMARY KEY,
    preferences JSONB NOT NULL DEFAULT '{}',
    learned_patterns JSONB DEFAULT '{}',
    interaction_style VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- ë©”ëª¨ë¦¬ ê´€ê³„ (ì—°ê²° ì •ë³´)
CREATE TABLE memory_relations (
    id SERIAL PRIMARY KEY,
    from_memory_type VARCHAR(20),
    from_memory_id INTEGER,
    to_memory_type VARCHAR(20),
    to_memory_id INTEGER,
    relation_type VARCHAR(50),
    strength FLOAT DEFAULT 0.5,

    INDEX idx_from (from_memory_type, from_memory_id),
    INDEX idx_to (to_memory_type, to_memory_id)
);
```

#### 2.2 ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ í™•ì¥
```python
class EnhancedMemoryService(SimpleMemoryService):
    """í™•ì¥ëœ ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤"""

    async def save_conversation_memory(
        self,
        user_id: int,
        session_id: str,
        summary: str,
        keywords: List[str],
        entities: Dict[str, Any],
        importance: float = 0.5
    ) -> int:
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥ (ì „ìš© í…Œì´ë¸”)"""
        memory = ConversationMemory(
            user_id=user_id,
            session_id=session_id,
            summary=summary,
            keywords=keywords,
            entities=entities,
            importance_score=importance
        )
        self.db.add(memory)
        await self.db.commit()
        return memory.id

    async def track_entity(
        self,
        user_id: int,
        entity_type: str,
        entity_name: str,
        properties: Dict
    ):
        """ì—”í‹°í‹° ì¶”ì  ë° ì—…ë°ì´íŠ¸"""
        # Upsert ë¡œì§
        existing = await self.get_entity(user_id, entity_type, entity_name)
        if existing:
            existing.properties.update(properties)
            existing.mention_count += 1
            existing.last_mentioned = datetime.now()
        else:
            entity = EntityMemory(
                user_id=user_id,
                entity_type=entity_type,
                entity_name=entity_name,
                properties=properties
            )
            self.db.add(entity)
        await self.db.commit()

    async def learn_user_preference(
        self,
        user_id: int,
        preference_type: str,
        value: Any
    ):
        """ì‚¬ìš©ì ì„ í˜¸ë„ í•™ìŠµ"""
        # ì ì§„ì  í•™ìŠµ ë¡œì§
        pass
```

---

### Phase 3: ë²¡í„° ê²€ìƒ‰ í†µí•© (3ì£¼)

**ëª©í‘œ: FAISS ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ êµ¬í˜„**

#### 3.1 ë²¡í„° ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸
```python
class VectorMemoryIndex:
    """FAISS ê¸°ë°˜ ë²¡í„° ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤"""

    def __init__(self, dimension: int = 1536):
        self.dimension = dimension
        self.index = faiss.IndexFlatL2(dimension)
        self.metadata = []  # memory_id ë§¤í•‘

    async def add_memory(
        self,
        memory_id: int,
        text: str,
        memory_type: str = "conversation"
    ):
        """ë©”ëª¨ë¦¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì¸ë±ìŠ¤ì— ì¶”ê°€"""
        # OpenAI Embedding
        embedding = await self.get_embedding(text)

        # FAISSì— ì¶”ê°€
        self.index.add(np.array([embedding]))
        self.metadata.append({
            "memory_id": memory_id,
            "memory_type": memory_type,
            "indexed_at": datetime.now()
        })

    async def search_similar(
        self,
        query: str,
        k: int = 5,
        threshold: float = 0.8
    ) -> List[Dict]:
        """ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰"""
        query_embedding = await self.get_embedding(query)

        # FAISS ê²€ìƒ‰
        distances, indices = self.index.search(
            np.array([query_embedding]), k
        )

        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if dist < threshold:
                meta = self.metadata[idx]
                results.append({
                    "memory_id": meta["memory_id"],
                    "memory_type": meta["memory_type"],
                    "similarity": 1.0 - dist,
                    "distance": dist
                })

        return results
```

#### 3.2 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
```python
async def hybrid_memory_search(
    self,
    query: str,
    user_id: int,
    search_mode: str = "hybrid"
) -> List[Dict]:
    """í‚¤ì›Œë“œ + ë²¡í„° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""

    results = []

    if search_mode in ["keyword", "hybrid"]:
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        keyword_results = await self.keyword_search(query, user_id)
        results.extend(keyword_results)

    if search_mode in ["vector", "hybrid"]:
        # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        vector_results = await self.vector_search(query, user_id)
        results.extend(vector_results)

    # ì¤‘ë³µ ì œê±° ë° ìŠ¤ì½”ì–´ ë³‘í•©
    merged = self.merge_results(results)

    # ì¤‘ìš”ë„ ê¸°ë°˜ ì¬ì •ë ¬
    return sorted(merged, key=lambda x: x["final_score"], reverse=True)
```

---

### Phase 4: ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ê´€ë¦¬ (4ì£¼)

**ëª©í‘œ: ìë™ í•™ìŠµ ë° ìµœì í™”**

#### 4.1 ë©”ëª¨ë¦¬ ì¤‘ìš”ë„ ìë™ ì¡°ì •
```python
class MemoryImportanceManager:
    """ë©”ëª¨ë¦¬ ì¤‘ìš”ë„ ë™ì  ê´€ë¦¬"""

    async def update_importance(self, memory_id: int):
        """ì ‘ê·¼ íŒ¨í„´ ê¸°ë°˜ ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸"""
        memory = await self.get_memory(memory_id)

        # íŒ©í„° ê³„ì‚°
        recency_factor = self.calculate_recency(memory.last_accessed)
        frequency_factor = memory.access_count / 100.0
        relevance_factor = await self.calculate_relevance(memory)

        # ê°€ì¤‘ í‰ê· 
        new_importance = (
            recency_factor * 0.3 +
            frequency_factor * 0.3 +
            relevance_factor * 0.4
        )

        memory.importance_score = min(1.0, new_importance)
        await self.db.commit()

    async def prune_old_memories(self, user_id: int):
        """ì˜¤ë˜ëœ/ì¤‘ìš”ë„ ë‚®ì€ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        threshold_date = datetime.now() - timedelta(days=90)

        # ì‚­ì œ ëŒ€ìƒ ì„ ì •
        candidates = await self.db.query(
            ConversationMemory
        ).filter(
            ConversationMemory.user_id == user_id,
            ConversationMemory.importance_score < 0.3,
            ConversationMemory.last_accessed < threshold_date
        ).all()

        # ì••ì¶• í›„ ì•„ì¹´ì´ë¸Œ
        for memory in candidates:
            await self.archive_memory(memory)
            await self.db.delete(memory)
```

#### 4.2 ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë©”ëª¨ë¦¬ ë¡œë”©
```python
class ContextAwareMemoryLoader:
    """ìƒí™© ì¸ì‹ ë©”ëª¨ë¦¬ ë¡œë”"""

    async def load_contextual_memories(
        self,
        user_id: int,
        current_query: str,
        context: Dict
    ) -> Dict[str, List]:
        """ì¿¼ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë©”ëª¨ë¦¬ ë¡œë”©"""

        # 1. ì˜ë„ ë¶„ì„
        intent = await self.analyze_intent(current_query)

        # 2. ê´€ë ¨ ì—”í‹°í‹° ì¶”ì¶œ
        entities = await self.extract_entities(current_query)

        # 3. ì‹œê°„ì  ê´€ë ¨ì„± ê³ ë ¤
        time_context = context.get("time_context", "recent")

        # 4. ê³„ì¸µë³„ ë©”ëª¨ë¦¬ ë¡œë”©
        memories = {
            "direct": [],      # ì§ì ‘ ê´€ë ¨
            "related": [],     # ê°„ì ‘ ê´€ë ¨
            "background": []   # ë°°ê²½ ì •ë³´
        }

        # ì§ì ‘ ê´€ë ¨ ë©”ëª¨ë¦¬ (ì—”í‹°í‹° ë§¤ì¹­)
        for entity in entities:
            entity_memories = await self.find_entity_memories(
                user_id, entity
            )
            memories["direct"].extend(entity_memories)

        # ê°„ì ‘ ê´€ë ¨ ë©”ëª¨ë¦¬ (ì˜ë¯¸ ìœ ì‚¬ë„)
        similar = await self.vector_search(
            current_query,
            user_id,
            threshold=0.7
        )
        memories["related"] = similar

        # ë°°ê²½ ì •ë³´ (ì„ í˜¸ë„, íŒ¨í„´)
        preferences = await self.get_user_preferences(user_id)
        memories["background"] = preferences

        return memories
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ê³¼ ì§€í‘œ

| ì§€í‘œ | í˜„ì¬ | Phase 1 | Phase 2 | Phase 3 | Phase 4 |
|------|------|---------|---------|---------|---------|
| **ë°ì´í„° ì¬ì‚¬ìš©ë¥ ** | 80% | 90% | 92% | 95% | 97% |
| **ì‘ë‹µ ì •í™•ë„** | 75% | 78% | 82% | 88% | 93% |
| **ì»¨í…ìŠ¤íŠ¸ ìœ ì§€** | 3 í„´ | 5 í„´ | 10 í„´ | 20 í„´ | ë¬´ì œí•œ |
| **ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì‹œê°„** | 100ms | 80ms | 60ms | 40ms | 20ms |
| **ìŠ¤í† ë¦¬ì§€ íš¨ìœ¨ì„±** | - | - | 30% ê°œì„  | 50% ê°œì„  | 70% ê°œì„  |

---

## ğŸ”§ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### ì¦‰ì‹œ ì ìš© (1ì£¼ ë‚´)
1. âœ… DataDetector í´ë˜ìŠ¤ êµ¬í˜„
2. âœ… ë©”íŠ¸ë¦­ ë¡œê¹… ì‹œìŠ¤í…œ
3. âœ… Checkpointer ë°ì´í„° ì¶”ì¶œ

### ë‹¨ê¸° ëª©í‘œ (1ê°œì›”)
1. ğŸ”„ ë©”ëª¨ë¦¬ í…Œì´ë¸” ìƒì„±
2. ğŸ”„ ì—”í‹°í‹° ì¶”ì  ì‹œì‘
3. ğŸ”„ ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰

### ì¤‘ê¸° ëª©í‘œ (3ê°œì›”)
1. ğŸ“‹ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ì„±
2. ğŸ“‹ ë©”ëª¨ë¦¬ ì¤‘ìš”ë„ ê´€ë¦¬
3. ğŸ“‹ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë¡œë”©

### ì¥ê¸° ëª©í‘œ (6ê°œì›”)
1. ğŸ¯ ì™„ì „ ìë™í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬
2. ğŸ¯ ë‹¤ì¤‘ ì‚¬ìš©ì ê²©ë¦¬
3. ğŸ¯ ë¶„ì‚° ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ

---

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **í˜„ì¬ ì‹œìŠ¤í…œì€ ê¸°ë°˜ì´ íƒ„íƒ„í•¨**
   - LangGraph 0.6ì˜ Checkpointer
   - PostgreSQL JSONB í™œìš©
   - 3-Tier êµ¬ì¡° ì´ë¯¸ êµ¬í˜„

2. **ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥í•œ ë¶€ë¶„ ë§ìŒ**
   - í‚¤ì›Œë“œ í™•ì¥ë§Œìœ¼ë¡œë„ í° íš¨ê³¼
   - ë©”íŠ¸ë¦­ ë¡œê¹…ìœ¼ë¡œ ìµœì í™” ê°€ëŠ¥
   - Checkpointer ë°ì´í„° í™œìš© ë¯¸í¡

3. **ë‹¨ê³„ì  ì ‘ê·¼ í•„ìš”**
   - Phase 1: í˜„ì¬ ì‹œìŠ¤í…œ ìµœì í™”
   - Phase 2: ì „ìš© ì €ì¥ì†Œ êµ¬ì¶•
   - Phase 3: ë²¡í„° ê²€ìƒ‰ í†µí•©
   - Phase 4: ì§€ëŠ¥í˜• ê´€ë¦¬

4. **ROI ê·¹ëŒ€í™” ì „ëµ**
   - ì‘ì€ ìˆ˜ì •, í° íš¨ê³¼ ìš°ì„ 
   - ê¸°ì¡´ ì½”ë“œ ìµœëŒ€í•œ í™œìš©
   - ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„ Action Items

### ì´ë²ˆ ì£¼ (Phase 1.1)
- [ ] DataDetector í´ë˜ìŠ¤ ì‘ì„± ë° í…ŒìŠ¤íŠ¸
- [ ] ë©”íŠ¸ë¦­ ë¡œê¹… ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì •

### ë‹¤ìŒ ì£¼ (Phase 1.2-1.3)
- [ ] Checkpointer ë°ì´í„° ì¶”ì¶œ êµ¬í˜„
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„±
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„

### ì´ë²ˆ ë‹¬ (Phase 2 ì‹œì‘)
- [ ] ë©”ëª¨ë¦¬ í…Œì´ë¸” DDL ì‘ì„±
- [ ] ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„
- [ ] EnhancedMemoryService í”„ë¡œí† íƒ€ì…

---

*ì´ ê³„íšì€ í˜„ì¬ êµ¬í˜„ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ìš©ì ì´ê³  ì ì§„ì ì¸ ê°œì„ ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.*
*ê° PhaseëŠ” ë…ë¦½ì ìœ¼ë¡œ ê°€ì¹˜ë¥¼ ì œê³µí•˜ë©°, ì¤‘ë‹¨ ì‹œì—ë„ ì´ì „ ë‹¨ê³„ì˜ ì„±ê³¼ëŠ” ìœ ì§€ë©ë‹ˆë‹¤.*